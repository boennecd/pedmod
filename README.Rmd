---
output: github_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  cache.path = "cache/README-",
  out.width = "100%", fig.width = 7, fig.height = 4
)
```

# pedmod: Pedigree Models

TODO: write about the package.

## Example

First, we source a file to get a function to simulate a data set with 
a maternal effect and a genetic effect:

```{r source_sim_file}
# souce the file to get the simulation function
source(system.file("gen-pedigree-data.R", package = "pedmod"))

# simulate a data set
set.seed(68167102)
dat <- sim_pedigree_data(n_families = 1000L)

# distribution of family sizes
table(sapply(dat$sim_data, function(x) length(x$y)))

# total number of observations
sum(sapply(dat$sim_data, function(x) length(x$y)))

# average event rate
mean(unlist(sapply(dat$sim_data, `[[`, "y")))

# TODO: show more about the families
```

Then we perform the model estimation:

```{r est_mod, cache=1}
# the true parameters are
dat$beta
dat$sc

# prepare the data to pass to the functions in the package
dat_arg <- lapply(dat$sim_data, function(x){
  # we need the following for each family: 
  #   y: the zero-one outcomes.
  #   X: the design matrix for the fixed effects. 
  #   scale_mats: list with the scale matrices for each type of effect.
  list(y = as.numeric(x$y), X = x$X,
       scale_mats = list(x$rel_mat, x$met_mat))
})

# create a C++ object
library(pedmod)
ll_terms <- get_pedigree_ll_terms(dat_arg, max_threads = 4L)

# get the starting values. This is very fast
y <- unlist(lapply(dat_arg, `[[`, "y"))
X <- do.call(rbind, lapply(dat_arg, `[[`, "X"))
start_fit <-  glm.fit(X, y, family = binomial("probit"))

# log-likelihood at the starting values without random effects
-sum(start_fit$deviance) / 2     
(beta <- start_fit$coefficients) # starting values for fixed effects 

# start at moderate sized scale parameters
sc <- rep(log(.2), 2)

# check log likelihood at the starting values. First we assign a function 
# to approximate the log likelihood and the gradient
fn <- function(par, seed = 1L, rel_eps = 1e-2, use_aprx = TRUE, 
               n_threads = 4L, indices = seq_along(dat_arg), 
               maxvls = 10000L){
  set.seed(seed)
  -eval_pedigree_ll(
    ll_terms, par = par, maxvls = maxvls, abs_eps = 0, rel_eps = rel_eps, 
    minvls = 1000L, use_aprx = use_aprx, n_threads = n_threads, 
    indices = indices)
}
gr <- function(par, seed = 1L, rel_eps = 1e-2, use_aprx = TRUE, 
               n_threads = 4L, indices = seq_along(dat_arg), 
               maxvls = 10000L){
  set.seed(seed)
  out <- -eval_pedigree_grad(
    ll_terms, par = par, maxvls = maxvls, abs_eps = 0, rel_eps = rel_eps, 
    minvls = 1000L, use_aprx = use_aprx, n_threads = n_threads, 
    indices = indices)
  structure(c(out), value = -attr(out, "logLik"))
}

# check output at the starting values
system.time(ll <- -fn(c(beta, sc)))
ll # the log likelihood at the starting values
system.time(gr_val <- gr(c(beta, sc)))
gr_val # the gradient at the starting values

# variance of the approximation
sd(sapply(1:25, function(seed) fn(c(beta, sc), seed = seed)))

# verify the gradient (may not be exactly equal due to MC error)
numDeriv::grad(fn, c(beta, sc))

# optimize the log likelihood approximation
system.time(opt <- optim(c(beta, sc), fn, gr, method = "BFGS"))
```

The output from the optimization is shown below:

```{r show_est_mod}
-opt$value      # the maximum log likelihood
opt$convergence # check convergence

# compare the estimated fixed effects with the true values
rbind(truth     = dat$beta, 
      estimated = head(opt$par, length(dat$beta)))

# compare estimated scale parameters with the true values
rbind(truth     = dat$sc, 
      estimated = exp(tail(opt$par, length(dat$sc))))
```

```{r, eval = FALSE, echo = FALSE}
#####
# performs stochastic gradient descent instead (using SVRG).
#
# Args:
#   par: starting value.
#   gr: gradient function which return the function value as an attributate
#       called value.
#   n_clust: number of clusters.
#   batch_size: number of observations in each batch.
#   maxit: maximum number of iteration.
#   lr: learning rate.
#   verbose: print output during the estimation.
#   decay: numeric scalar used to decrease the learning rate.
#   ...: arguments passed to gr.
svrg <- function(par, gr, n_clust, batch_size, maxit = 10L, 
                 lr, verbose = FALSE, decay = .98, ...){
  indices <- sample.int(n_clust, replace = FALSE) - 1L
  blocks <- tapply(indices, (seq_along(indices) - 1L) %/% batch_size,
                   identity, simplify = FALSE)

  n_blocks <- length(blocks)
  n_par <- length(par)
  estimates <- matrix(NA_real_, n_par, maxit + 1L)
  fun_vals <- numeric(maxit + 1L)
  estimates[, 1L] <- par

  lr_use <- lr / decay
  V_mult <- qnorm(1 - .99 / maxit)
  for(k in 1:maxit + 1L){
    old_val <- estimates[, k - 1L]
    old_grs <- sapply(1:n_blocks - 1L, function(ii){
      idx_b <- (ii %% n_blocks) + 1L
      res_old <- gr(old_val, indices = blocks[[idx_b]], ...)
      c(attr(res_old, "value"), res_old)
    })

    fun_vals[k - 1L] <- sum(old_grs[1, ])
    old_grs <- old_grs[-1L, , drop = FALSE ]
    old_gr <- rowSums(old_grs) / n_blocks

    lr_use <- lr_use * decay
    for(ii in 1:n_blocks - 1L){
      idx_b <- (ii %% n_blocks) + 1L
      res <- gr(par, indices = blocks[[idx_b]], ...)
      fun_vals[k] <- fun_vals[k] + attr(res, "value")
      dir <- res - old_grs[, ii + 1L] + old_gr

      par <- par - lr_use * dir
    }

    estimates[, k] <- par

    if(verbose)
      cat(
        sprintf("End if iteration %4d with learning rate %.8f", k - 1L,
                lr_use),
        sprintf("Log marginal likelihood approximation is %12.2f", fun_vals[k]),
        sprintf("Previous approximate gradient norm was %14.2f\n",
                n_blocks * norm(as.matrix(old_gr))),
        sep = "\n")

    old_v <- fun_vals[k - 1L]
    new_v <- fun_vals[k]
  }

  list(result = par, fun_vals = fun_vals[2:k],
       estimates = estimates[, 2:k, drop = FALSE])
}

set.seed(1)
svrg_res <- svrg(c(beta, sc), gr = gr, n_clust = length(dat_arg), 
                 batch_size = 200L, lr = 2e-3, verbose = TRUE, maxit = 50L)
```

### Using ADAM
We use stochastic gradient descent with the ADAM method in this section. 
We define a function to do so below and use it to estimate the model.

```{r use_adam, cache = 1}
#####
# performs stochastic gradient descent (using ADAM).
#
# Args:
#   par: starting value.
#   gr: function to evaluate the log marginal likelihood.
#   n_clust: number of observation.
#   n_blocks: number of blocks.
#   maxit: maximum number of iteration.
#   seed: seed to use.
#   epsilon, alpha, beta_1, beta_2: ADAM parameters.
#   maxvls: maximum number of samples to draw.
#   verbose: print output during the estimation.
#   ...: arguments passed to gr.
adam <- function(par, gr, n_clust, n_blocks, maxit = 10L,
                 seed = 1L, epsilon = 1e-8, alpha = .001, beta_1 = .9,
                 beta_2 = .999, maxvls = rep(10000L, maxit), 
                 verbose = FALSE, ...){
  grp_dummy <- (seq_len(n_clust) - 1L) %% n_blocks
  n_par <- length(par)
  m <- v <- numeric(n_par)
  fun_vals <- numeric(maxit)
  estimates <- matrix(NA_real_, n_par, maxit)
  i <- -1L

  for(k in 1:maxit){
    # sample groups
    indices <- sample.int(n_clust, replace = FALSE) - 1L
    blocks <- tapply(indices, grp_dummy, identity, simplify = FALSE)
    
    for(ii in 1:n_blocks){
      i <- i + 1L
      idx_b <- (i %% n_blocks) + 1L
      m_old <- m
      v_old <- v
      res <- gr(par, indices = blocks[[idx_b]], maxvls = maxvls[k])
      fun_vals[(i %/% n_blocks) + 1L] <-
        fun_vals[(i %/% n_blocks) + 1L] + attr(res, "value")
      res <- c(res)

      m <- beta_1 * m_old + (1 - beta_1) * res
      v <- beta_2 * v_old + (1 - beta_2) * res^2

      m_hat <- m / (1 - beta_1^(i + 1))
      v_hat <- v / (1 - beta_2^(i + 1))

      par <- par - alpha * m_hat / (sqrt(v_hat) + epsilon)
    }
    
    if(verbose){
      cat(sprintf("Ended iteration %4d. Running estimate of the function value is: %14.2f\n", 
                  k, fun_vals[k]))
      cat("Parameter estimates are:\n")
      cat(capture.output(print(par)), sep = "\n")
      cat("\n")
    }

    estimates[, k] <- par
  }

  list(par = par, estimates = estimates, fun_vals = fun_vals)
}

#####
# use the function
# assign the maximum number of samples we will use
maxit <- 100L
minvls <- 250L
maxpts <- 10000L
maxpts_use <- exp(seq(log(2 * minvls), log(maxpts), length.out = maxit))

# show the maximum number of samples we use
par(mar = c(5, 4, 1, 1))
plot(maxpts_use, pch = 16, xlab = "Iteration number", bty = "l",
     ylab = "Maximum number of samples", ylim = range(0, maxpts_use))

set.seed(1)
system.time(
  adam_res <- adam(c(beta, sc), gr = gr, n_clust = length(dat_arg), 
                   n_blocks = 10L, alpha = 1e-2, maxit = maxit, 
                   verbose = FALSE, maxvls = maxpts_use, 
                   minvls = minvls))
```

The result is shown below.

```{r res_adam}
-fn(adam_res$par) # the maximum log likelihood

# compare the estimated fixed effects with the true values
rbind(truth     = dat$beta, 
      estimated = head(adam_res$par, length(dat$beta)))

# compare estimated scale parameters with the true values
rbind(truth     = dat$sc, 
      estimated = exp(tail(adam_res$par, length(dat$sc))))
```

### The Multivariate Normal CDF Approximation

We compare the multivariate normal CDF approximation in this section 
with the approximation from the mvtnorm package. The same algorithm is used 
but the version in this package is re-written in C++ and differs slightly.
Moreover, we have implemented an approximation of the standard normal CDF
and its inverse which reduces the computation time as we will show below.

```{r compare_w_mvtnorm, cache = 1}
#####
# settings for the simulation study
library(mvtnorm)
library(pedmod)
library(microbenchmark)
set.seed(78459126)
n <- 5L         # number of variables to interate out
rel_eps <- 1e-4 # the relative error to use

#####
# run the simulation study
sim_res <- replicate(expr = {
  # simulate covariance matrix and the upper bound
  S <- drop(rWishart(1L, 2 * n, diag(n) / 2 / n))
  u <- rnorm(n)
  
  # function to use pmvnorm
  use_mvtnorm <- function(rel_eps)
    pmvnorm(upper = u, sigma = S, algorithm = GenzBretz(
      abseps = 0, releps = rel_eps, maxpts = 1e7))
  
  # function to use this package
  use_mvndst <- function(use_aprx = FALSE)
    mvndst(lower = rep(-Inf, n), upper = u, mu = rep(0, n), 
           sigma = S, use_aprx = use_aprx, abs_eps = 0, rel_eps = rel_eps,
           maxvls = 1e7)

  # get a very precise estimate
  truth <- use_mvtnorm(rel_eps / 100)
  
  # computes the error with repeated approximations and compute the time it
  # takes
  n_rep <- 5L
  run_n_time <- function(expr){
    expr <- substitute(expr)
    ti <- get_nanotime()
    res <- replicate(n_rep, eval(expr))
    ti <- get_nanotime() - ti
    err <- (res - truth) / truth
    c(SE = sqrt(sum(err^2) / n_rep), time = ti / n_rep / 1e9)
  }
  
  mvtnorm_res        <- run_n_time(use_mvtnorm(rel_eps))
  mvndst_no_aprx_res <- run_n_time(use_mvndst(FALSE))
  mvndst_w_aprx_res  <- run_n_time(use_mvndst(TRUE))
  
  # return 
  rbind(mvtnorm            = mvtnorm_res, 
        `mvndst (no aprx)` = mvndst_no_aprx_res, 
        `mvndst (w/ aprx)` = mvndst_w_aprx_res)
}, n = 100, simplify = "array")
```

They have about the same average relative error as expected:

```{r show_averge_rel_err}
rowMeans(sim_res[, "SE", ])
par(mar = c(5, 4, 1, 1))
boxplot(t(sim_res[, "SE", ]))

# without extreme values
sum(keep <- colSums(sim_res[, "SE", ] > .5) < 1)
rowMeans(sim_res [, "SE", keep])
boxplot(t(sim_res[, "SE", keep]))
```

The new implementation is faster when the approximation is used:

```{r use_new_impl}
rowMeans(sim_res[, "time", ])
par(mar = c(5, 4, 1, 1))
boxplot(t(sim_res[, "time", ]))
```
