---
bibliography: ../ref.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  cache.path = "cache/pedmod-",
  out.width = "100%", fig.width = 7, fig.height = 4,
  message = FALSE, warning = FALSE, error = FALSE, 
  dpi = 50)
options(digits = 4)
```

# pedmod: Pedigree Models

[![R-CMD-check](https://github.com/boennecd/pedmod/workflows/R-CMD-check/badge.svg)](https://github.com/boennecd/pedmod/actions)
[![](https://www.r-pkg.org/badges/version/pedmod)](https://CRAN.R-project.org/package=pedmod)
[![CRAN RStudio mirror downloads](http://cranlogs.r-pkg.org/badges/pedmod)](https://CRAN.R-project.org/package=pedmod)

The pedmod package provides functions to estimate models for pedigree data. 
Particularly, the package provides functions to estimate mixed models of 
the form:

$$\begin{align*}
Y_{ij} \mid \epsilon_{ij} = e 
  &\sim \text{Bin}(\Phi(\vec\beta^\top\vec x_{ij} + e), 1) \\
\vec\epsilon_i = (\epsilon_{i1}, \dots, \epsilon_{in_i})^\top &\sim
  N^{(n_i)}\left(\vec 0, \sum_{l = 1}^K\sigma_l^2 C_{il}
  \right)
\end{align*}$$

where $Y_{ij}$ is the binary outcome of interest for individual $j$ in 
family/cluster $i$, $\vec x_{ij}$ is the individual's known covariates, 
$\Phi$ is the standard normal distribution's CDF, and 
$\text{Bin}$ implies a binomial distribution such if 
$z\sim \text{Bin}(p, n)$ then the density of $z$ is:

$$f(z) = \begin{pmatrix} n \\ z \end{pmatrix}p^z(1 - p)^{n-z}$$

A different and equivalent way of writing the model is as:

$$\begin{align*}
Y_{ij} \mid \epsilon_{ij} = e 
  &= \begin{cases}
    1 & \vec\beta^\top\vec x_{ij} + e > 0 \\
    0 & \text{otherwise}
    \end{cases} \\
\vec\epsilon_i = (\epsilon_{i1}, \dots, \epsilon_{in_i})^\top &\sim
  N^{(n_i)}\left(\vec 0, I_{n_i} + \sum_{l = 1}^K\sigma_l^2 C_{il}
  \right)
\end{align*}$$

where $I_{n_i}$ is the $n_i$ dimensional identity matrix which comes from the 
unshared/individual specific random effect. This effect is always included. 
The models are commonly known as liability threshold models or mixed probit 
models.

The $C_{il}$s are known scale/correlation matrices where each of the $l$'th 
types correspond to a type of effect. An arbitrary number of such matrices can 
be passed to include e.g. a genetic effect, a maternal effect, a paternal, an 
effect of a shared adult environment etc. Usually, these matrices are 
correlation matrices as this simplifies later interpretation and we will assume 
that all the matrices are correlation matrices. A 
typical example is that $C_{il}$ is two times the kinship matrix in which 
case we call:

$$\frac{\sigma_l^2}{1 + \sum_{k = 1}^K\sigma_k^2}$$

the heritability. That is, the proportion of the variance attributable to the 
the $l$'th effect which in this case is the direct genetic effect. 
The scale parameters, the $\sigma_k^2$s, may be the 
primary interest in an analysis. The scale in the model cannot be identified. 
That is, an equivalent model is:

$$\begin{align*}
Y_{ij} \mid \epsilon_{ij} = e 
  &= \begin{cases}
    1 & \sqrt\phi\vec\beta^\top\vec x_{ij} + e > 0 \\
    0 & \text{otherwise}
    \end{cases} \\
\vec\epsilon_i = (\epsilon_{i1}, \dots, \epsilon_{in_i})^\top &\sim
  N^{(n_i)}\left(\vec 0, 
  \phi\left(I_{n_i} + \sum_{l = 1}^K\sigma_l^2 C_{il}\right)
  \right)
\end{align*}$$

for any $\phi > 0$. A common option other than $\phi = 1$ is to set 
$\phi = (1 + \sum_{l = 1}^K \sigma_l^2)^{-1}$. This has the effect that  

$$\frac{\sigma_l^2}{1 + \sum_{k = 1}^K\sigma_k^2} = \phi\sigma_l^2$$

is the proportion of variance attributable to the $l$'th effect (assuming all 
$C_{il}$ matrices are correlation matrices). Moreover, $\phi$ is the proportion 
of variance attributable to the individual specific effect. 

The parameterizations used in the package are $\phi = 1$
which we call the direct parameterizations and 
$(1 + \sum_{l = 1}^K \sigma_l^2)^{-1}$ which we call the standardized 
parameterizations. 
The latter have the advantage that it is easier to interpret as the scale 
parameters are the proportion of variance attributable to each effect 
(assuming that only correlation matrices are used) and 
the $\sqrt\phi\vec\beta$ are often very close the estimate from a GLM 
(that is, the model without the other random effects) 
when the covariates are unrelated to random effects that are added to the model.
The latter makes it easy to find starting values. 

For the above reason, two parameterization are used. For
the direct parameterization where $\phi = 1$, we work directly with $\vec\beta$, 
and we use $\theta_l = \log\sigma_l^2$. For the
standardized parameterization
where $\phi = (1 + \sum_{l = 1}^K \sigma_l^2)^{-1}$, we work with
$\phi = (1 + \sum_{l = 1}^K \sigma_l^2)^{-1}$,
$\vec\gamma = \sqrt\phi\vec\beta$, and 

$$\phi\sigma_l^2 = \frac{\exp(\psi_l)}{1 +\sum_{l = 1}^k\exp(\psi_l)}\Leftrightarrow\sigma_l^2 = \exp(\psi_l).$$

This package provides randomized quasi-Monte Carlo methods to approximate 
the log marginal likelihood for these types of 
models with an arbitrary number scale matrices, $K$, and the derivatives 
with respect to $(\vec\beta^\top, 2\log\sigma_1,\dots, 2\log\sigma_K)^\top$
(that is, we work with $\psi_k = 2\log\sigma_k$) or 
$(\vec\gamma^\top, \psi_1, \dots, \psi_K)$.

In some cases, it may be hypothesized that some individuals are less effected by 
e.g. their genes than others. A model to incorporate such effects is 
implemented in the `pedigree_ll_terms_loadings` function.
See the [Individual Specific Loadings](#individual-specific-loadings) section 
for details and examples.

We have re-written the Fortran code by @Genz02 in C++, made it easy to 
extend from a log marginal likelihood approximation to other approximations 
such as the derivatives, and added less precise but faster approximations 
of the $\Phi$ and $\Phi^{-1}$. Our own experience suggests that using the 
latter has a small effect on the precision of the result but can yield 
substantial reduction in computation times for moderate sized 
families/clusters.

The approximation by @Genz02 have already been used to estimate these 
types of models [@Pawitan04]. However, not having the gradients may slow 
down estimation substantially. Moreover, our implementation supports 
computation in parallel which is a major advantage given the 
availability of multi-core processors. 

Since the implementation is easy to extend, possible extensions are: 

 1. Survival times using mixed generalized survival models [@Liu17] with 
    a similar random effect structure as the model shown above. This way, 
    one avoids dichotomizing outcomes and can account for censoring.
 2. Generalized linear mixed model with binary, binomial, ordinal, or 
    multinomial outcomes with a probit link. The method we use here may be
    beneficial if the number of random effects per cluster is not much 
    smaller then the number observations in each cluster. This is used for 
    imputation in the mdgc package.
    
## Installation
The package can be installed from GitHub by calling:

```{r how_to_install, eval = FALSE}
remotes::install_github("boennecd/pedmod", build_vignettes = TRUE)
```

The package can also be installed from CRAN by calling:

```{r cran_how_to_install, eval = FALSE}
install.packages("pedmod")
```

The code benefits from being build with automatic vectorization so having e.g. 
`-O3` in the `CXX14FLAGS` flags in your Makevars file may be useful.

## Example

We start with a simple example only with a direct genetic effect. We have one 
type of family which consists of two couples which are related through one 
of the parents being siblings. The family is shown below.

```{r setup_simple}
# create the family we will use
fam <- data.frame(id = 1:10, sex = rep(1:2, 5L),
                  father = c(NA, NA, 1L, NA, 1L, NA, 3L, 3L, 5L, 5L), 
                  mother = c(NA, NA, 2L, NA, 2L, NA, 4L, 4L, 6L, 6L))

# plot the pedigree
library(kinship2)
ped <- with(fam, pedigree(id = id, dadid = father, momid = mother, sex = sex))
plot(ped)
```

We set the scale matrix to be two times the kinship matrix to model the direct
genetic effect. Each individual also has a standard normally distributed 
covariate and a binary covariate. Thus, we can simulate a data set with a 
function like:

```{r assign_sim_dat}
# simulates a data set. 
# 
# Args:
#   n_fams: number of families.
#   beta: the fixed effect coefficients.
#   sig_sq: the scale parameter.
sim_dat <- function(n_fams, beta = c(-3, 1, 2), sig_sq = 3){
  # setup before the simulations
  Cmat <- 2 * kinship(ped)
  n_obs <- NROW(fam)
  Sig <- diag(n_obs) + sig_sq * Cmat
  Sig_chol <- chol(Sig)
  
  # simulate the data
  out <- replicate(
    n_fams, {
      # simulate covariates
      X <- cbind(`(Intercept)` = 1, Continuous = rnorm(n_obs), 
                 Binary = runif(n_obs) > .5)
      
      # assign the linear predictor + noise
      eta <- drop(X %*% beta) + drop(rnorm(n_obs) %*% Sig_chol)
      
      # return the list in the format needed for the package
      list(y = as.numeric(eta > 0), X = X, scale_mats = list(Cmat))
    }, simplify = FALSE)
  
  # add attributes with the true values and return 
  attributes(out) <- list(beta = beta, sig_sq = sig_sq)
  out
}
```

The model is 

$$\begin{align*}
 Y_{ij} &= \begin{cases} 1 & \beta_0 + \beta_1 X_{ij} + \beta_2 B_{ij} + G_{ij} + R_{ij} > 0 \\ 0 & \text{otherwise} \end{cases} \\
 X_{ij} &\sim N(0, 1) \\
 B_{ij} &\sim \text{Bin}(0.5, 1) \\
 (G_{i1}, \dots, G_{in_{i}})^\top &\sim N^{(n_i)}(\vec 0, \sigma^2 C_{i1}) \\
 R_{ij} &\sim N(0, 1)\end{align*}$$
 
where $C_{i1}$ is two times the kinship matrix and $X_{ij}$ and 
$B_{ij}$ are observed covariates. We can now estimate the model with a simulated 
data set as follows:

```{r est_simple, cache = 1}
# simulate a data set
set.seed(27107390)
dat <- sim_dat(n_fams = 400L)

# perform the optimization. We start with finding the starting values
library(pedmod)
ll_terms <- pedigree_ll_terms(dat, max_threads = 4L)
system.time(start <- pedmod_start(ptr = ll_terms, data = dat, n_threads = 4L))

# log likelihood without the random effects and at the starting values
start$logLik_no_rng
start$logLik_est # this is unreliably/imprecise

# estimate the model
system.time(
  opt_out <- pedmod_opt(
    ptr = ll_terms, par = start$par, abs_eps = 0, use_aprx = TRUE, 
    n_threads = 4L, 
    maxvls = 25000L, rel_eps = 1e-3, minvls = 5000L))
```

The results of the estimation are shown below:

```{r est_simple_res}
# parameter estimates versus the truth
rbind(opt_out       = head(opt_out$par, -1), 
      opt_out_quick = head(start  $par, -1), 
      truth         = attr(dat, "beta"))
c(opt_out       = exp(tail(opt_out$par, 1)), 
  opt_out_quick = exp(tail(start  $par, 1)), 
  truth         = attr(dat, "sig_sq"))

# log marginal likelihoods
print(start   $logLik_est, digits = 8) # this is unreliably/imprecise
print(-opt_out$value     , digits = 8)
```

We emphasize that we set the `rel_eps` parameter to `1e-3` above 
which perhaps is fine for this size of a data set but may not be fine for larger 
data sets for the following reason. Suppose that we have $i = 1,\dots,m$ 
families/clusters and suppose that we estimate the log likelihood term for each 
family with a variance of $\zeta$. This implies that the variance of the 
log likelihood for all the families is $\zeta m$. Thus, the 
precision we require for each family's log likelihood term needs to be
proportional to $\mathcal O(m^{-1/2})$ if we want a fixed number of precise 
digits
for the log likelihood for all number of families. The latter is important e.g. 
for the profile likelihood curve we compute later and also for the line search
used by some optimization methods. Thus, one may need to reduce
`rel_eps` and increase `maxvls` when there are many families.

We can construct standard errors by computing the Hessian using the 
`eval_pedigree_hess` function as shown below. Like `eval_pedigree_grad`, 
the `eval_pedigree_hess` functions takes in the log of the scale parameters but
the Hessian is computed on the scale of the scale parameters. 

```{r hess_est_simple_res, cache = 1}
set.seed(1)
system.time(hess <- eval_pedigree_hess(
  ptr = ll_terms, par = opt_out$par, maxvls = 25000L, minvls = 5000L, abs_eps = 0, 
  rel_eps = 1e-4, do_reorder = TRUE, use_aprx = FALSE, n_threads = 4L))

# the gradient is quite small
sqrt(sum(attr(hess, "grad")^2))

# show parameter estimates along with standard errors
rbind(Estimates = opt_out$par, 
      SE = sqrt(diag(attr(hess, "vcov"))))
rbind(Estimates = c(head(opt_out$par, -1), exp(tail(opt_out$par, 1))), 
      SE = sqrt(diag(attr(hess, "vcov_org"))))
```

We may want to report estimates with the proportion of variances and 
the standardized fixed effects coefficients which we show later. This can be 
done by applying the delta method. An example is given below.

```{r std_props_ex}
# computes the standardized coefficients and proportion of variances. The 
# covariance matrix is computed using the delta method.
# 
# Args:
#   par: the parameter estimates. 
#   n_scales: the number of scale parameters.
#   hess: the output from eval_pedigree_hess
std_prop_estimates <- function(par, n_scales, hess = NULL){
  # transform the parameter estimates
  n_par <- length(par)
  n_fixef <- n_par - n_scales
  idx_scale <- seq_len(n_scales) + n_fixef
  par[idx_scale] <- exp(par[idx_scale])
  total_var <- 1 + sum(par[idx_scale])
  
  denom <- sqrt(total_var)
  d_denom <- -1/(2 * denom * total_var)
  par_out <- c(par[-idx_scale] / denom, par[idx_scale] / total_var)
  
  if(!is.null(hess)){
    # compute the Jacobian from par to par_out
    jac <- matrix(0, n_par, n_par)
    n_fixed <- n_par - n_scales 
    for(i in seq_len(n_fixed)){
      jac[i, i] <- 1 / denom
      jac[i, idx_scale] <- par[i] * d_denom
    }
    
    for(i in seq_len(n_scales)){
      jac[idx_scale[i], idx_scale   ] <- -par[idx_scale[i]] / total_var^2
      jac[idx_scale[i], idx_scale[i]] <- 
        jac[idx_scale[i], idx_scale[i]] + 1 / total_var
    }
    
    # compute the Hessian using the delta method
    vcov_var <- tcrossprod(jac %*% attr(hess, "vcov_org"), jac)
  } else 
    vcov_var <- NULL
  
  list(par = par_out, vcov_var = vcov_var)
}

# show the transformed estimates along with standard errors
std_prop <- std_prop_estimates(opt_out$par, n_scales = 1L, hess = hess)
rbind(
  Truth = std_prop_estimates(
    c(attr(dat, "beta"), log(attr(dat, "sig_sq"))), 1)$par,
  Estimates = std_prop$par, SE = sqrt(diag(std_prop$vcov_var)))
```

### Minimax Tilting

The minimax tilting method suggested by @Botev17 is also implemented. The 
method is more numerically stable when the marginal likelihood terms are small
(for instance with large clusters)
or for certain problems. However, there is some overhead in the implementation 
of the method as underflow becomes an issue. This requires more care which
increases the computation time. 

We estimate the model below with the minimax tilting using the `use_tilting` 
argument. 

```{r tilt_est_simple, cache = 1}
# perform the optimization. We start with finding the starting values
set.seed(60941821)
system.time(
  start_tilt <- pedmod_start(
    ptr = ll_terms, data = dat, n_threads = 4L, use_tilting = TRUE, 
    use_aprx = FALSE))

# estimate the model
system.time(
  opt_out_tilt <- pedmod_opt(
    ptr = ll_terms, par = start_tilt$par, abs_eps = 0, use_aprx = FALSE, 
    n_threads = 4L, use_tilting = TRUE,
    maxvls = 25000L, rel_eps = 1e-3, minvls = 5000L))
```

The results of the estimation are shown below:

```{r tilt_est_simple_res}
# parameter estimates versus the truth
rbind(opt_out_tilt = head(opt_out_tilt$par, -1),
      opt_out      = head(opt_out$par     , -1),
      truth        = attr(dat, "beta"))
c(opt_out_tilt = exp(tail(opt_out_tilt$par, 1)),
  opt_out      = exp(tail(opt_out$par, 1)), 
  truth        = attr(dat, "sig_sq"))

# log marginal likelihoods
print(start        $logLik_est, digits = 8) # this is unreliably/imprecise
print(start_tilt   $logLik_est, digits = 8) # this is unreliably/imprecise

print(-opt_out     $value     , digits = 8)
print(-opt_out_tilt$value     , digits = 8)
```

### Different Optimizer

As the gradient is an approximation, some nonlinear optimizer may give better 
results than others. We illustrate this below by using the `nlminb` function.

```{r nlminb_tilt_est_simple, cache = 1}
# create a wrapper function
nlminb_wrapper <- function(
  par, fn, gr = NULL, control = list(eval.max = 1000L, iter.max = 1000L), ...){
  out <- nlminb(
    start = par, objective = fn, gradient = gr, control = control, ...)
  within(out, {
    counts <- evaluations
    value <- objective
  })
}

# estimate the model
system.time(
  opt_out_tilt_nlminb <- pedmod_opt(
    ptr = ll_terms, par = start_tilt$par, abs_eps = 0, use_aprx = FALSE, 
    n_threads = 4L, use_tilting = TRUE,
    maxvls = 25000L, rel_eps = 1e-3, minvls = 5000L, opt_func = nlminb_wrapper))
```

The results of the estimation are shown below:

```{r nlminb_tilt_est_simple_res}
# parameter estimates versus the truth
rbind(opt_out_tilt_nlminb = head(opt_out_tilt_nlminb$par, -1),
      opt_out_tilt        = head(opt_out_tilt$par, -1),
      opt_out             = head(opt_out$par     , -1),
      truth               = attr(dat, "beta"))
c(opt_out_tilt_nlminb = exp(tail(opt_out_tilt_nlminb$par, 1)),
  opt_out_tilt        = exp(tail(opt_out_tilt$par, 1)),
  opt_out             = exp(tail(opt_out$par, 1)), 
  truth               = attr(dat, "sig_sq"))

# log marginal likelihoods
print(-opt_out            $value, digits = 8)
print(-opt_out_tilt       $value, digits = 8)
print(-opt_out_tilt_nlminb$value, digits = 8)
```

### Alternative Parameterization

As an alternative to the direct parameterization we use above, we can also use
the standardized parameterization. Below are some illustrations which you may 
skip. 

```{r std_est_simple, cache = 1}
#####
# transform the parameters and check that we get the same likelihood
std_par <- direct_to_standardized(opt_out$par, n_scales = 1L)
std_par # the standardized parameterization
opt_out$par # the direct parameterization 

# we can map back as follows
par_back <- standardized_to_direct(std_par, n_scales = 1L)
all.equal(opt_out$par, par_back, check.attributes = FALSE)
# the proportion of variance of each effect
attr(par_back, "variance proportions") 

# the proportion match
exp(tail(opt_out$par, 1)) / (exp(tail(opt_out$par, 1)) + 1)

# compute the likelihood with either parameterization
set.seed(1L)
eval_pedigree_ll(ptr = ll_terms, par = opt_out$par, maxvls = 10000L, 
                 minvls = 1000L, rel_eps = 1e-3, use_aprx = TRUE, abs_eps = 0)
set.seed(1L)
eval_pedigree_ll(ptr = ll_terms, par = std_par    , maxvls = 10000L, 
                 minvls = 1000L, rel_eps = 1e-3, use_aprx = TRUE, abs_eps = 0, 
                 standardized = TRUE)

# we can also get the same gradient with an application of the chain rule
jac <- attr(
  standardized_to_direct(std_par, n_scales = 1L, jacobian = TRUE), 
  "jacobian")

set.seed(1L)
g1 <- eval_pedigree_grad(ptr = ll_terms, par = opt_out$par, maxvls = 10000L, 
                         minvls = 1000L, rel_eps = 1e-3, use_aprx = TRUE, 
                         abs_eps = 0)
set.seed(1L)
g2 <- eval_pedigree_grad(ptr = ll_terms, par = std_par, maxvls = 10000L, 
                         minvls = 1000L, rel_eps = 1e-3, use_aprx = TRUE, 
                         abs_eps = 0, standardized = TRUE)
all.equal(drop(g1 %*% jac), g2, check.attributes = FALSE)
```

The model can also be estimated with the standardized parameterization:

```{r continued_std_est_simple, cache = 1}
# perform the optimization. We start with finding the starting values
system.time(start_std <- pedmod_start(
  ptr = ll_terms, data = dat, n_threads = 4L, standardized = TRUE))

# the starting values are close
standardized_to_direct(start_std$par, n_scales = 1L)
start$par

# this may have required different number of gradient and function evaluations
start_std$opt$counts
start    $opt$counts

# estimate the model
system.time(
  opt_out_std <- pedmod_opt(
    ptr = ll_terms, par = start_std$par, abs_eps = 0, use_aprx = TRUE, 
    n_threads = 4L, standardized = TRUE,
    maxvls = 25000L, rel_eps = 1e-3, minvls = 5000L))

# we get the same
standardized_to_direct(opt_out_std$par, n_scales = 1L)
opt_out$par

# this may have required different number of gradient and function evaluations
opt_out_std$counts
opt_out    $counts
```

### Stochastic Quasi-Newton Method

The package includes a stochastic quasi-Newton method which can be used to 
estimate the model. This may be useful for larger data sets or in situations 
where `pedmod_opt` "get stuck" near a maximum. The reason for the latter is
presumably that `pedmod_opt` (by default) uses the BFGS method which does 
not assume any noise in the gradient or the function. We give an example below 
of how to use the stochastic quasi-Newton method provided through the 
`pedmod_sqn` function.

```{r pre_use_pedmod_psqn, echo=FALSE}
ll_terms <- pedigree_ll_terms(dat, max_threads = 4L)
```

```{r use_pedmod_sqn, cache = 1, fig.keep = "last"}
# fit the model with the stochastic quasi-Newton method
set.seed(46712994)
system.time(
  sqn_out <- pedmod_sqn(
    ptr = ll_terms, par = start$par, abs_eps = 0, use_aprx = TRUE, 
    n_threads = 4L, rel_eps = 1e-3, step_factor = .1, maxvls = 25000L, 
    minvls = 1000L, n_it = 400L, n_grad_steps = 10L, n_grad = 100L, 
    n_hess = 400L))

# show the log marginal likelihood
ll_wrapper <- function(x)
  eval_pedigree_ll(
    ptr = ll_terms, x, maxvls = 50000L, minvls = 1000L, abs_eps = 0, 
    rel_eps = 1e-4, n_threads = 4L)
print(ll_wrapper(sqn_out$par), digits = 8)
print(ll_wrapper(opt_out$par), digits = 8)

# compare the parameters
rbind(optim = opt_out$par, 
      sqn   = sqn_out$par)

# plot the marginal log likelihood versus the iteration number
lls <- apply(sqn_out$omegas, 2L, ll_wrapper)
par(mar = c(5, 5, 1, 1))
plot(lls, ylab = "Log marginal likelihood", bty = "l", pch = 16,
     xlab = "Hessian updates")
lines(smooth.spline(seq_along(lls), lls))
grid()
# perhaps we could have used fewer samples in each iteration
set.seed(46712994)
system.time(
  sqn_out_few <- pedmod_sqn(
    ptr = ll_terms, par = start$par, abs_eps = 0, use_aprx = TRUE, 
    n_threads = 4L, rel_eps = 1e-3, step_factor = .1, maxvls = 25000L, 
    minvls = 1000L, n_grad_steps = 20L,
    # we take more iterations
    n_it = 2000L, 
    # but use fewer samples in each iteration
    n_grad = 20L, n_hess = 100L))

# compute the marginal log likelihood and compare the parameter estimates
print(ll_wrapper(sqn_out_few$par), digits = 8)

rbind(optim       = opt_out    $par, 
      sqn         = sqn_out    $par, 
      `sqn (few)` = sqn_out_few$par)
```

### Profile Likelihood Curve

We can compute a profile likelihood curve like this:

```{r simple_ex_profile_likelihood, cache = 1}
# assign the scale parameter at which we will evaluate the profile likelihood
rg <- range(exp(tail(opt_out$par, 1) / 2) * c(.5, 2),
            sqrt(attr(dat, "sig_sq")) * c(.9, 1.1))
sigs <- seq(rg[1], rg[2], length.out = 10)
sigs <- sort(c(sigs, exp(tail(opt_out$par, 1) / 2)))

# compute the profile likelihood
ll_terms <- pedigree_ll_terms(dat, max_threads = 4L)
pl_curve_res <- lapply(sigs, function(sig){
  # set the parameters to pass
  beta <- start$beta_no_rng
  sig_sq_log <- 2 * log(sig)
  beta_scaled <- beta * sqrt(1 + sig^2)
  
  # optimize like before but using the fix argument
  opt_out_quick <- pedmod_opt(
    ptr = ll_terms, par = c(beta_scaled, sig_sq_log), maxvls = 1000L, 
    abs_eps = 0, rel_eps = 1e-2, minvls = 100L, use_aprx = TRUE, n_threads = 4L, 
    fix = length(beta) + 1L)
  opt_out <- pedmod_opt(
    ptr = ll_terms, par = c(opt_out_quick$par, sig_sq_log), abs_eps = 0, 
    use_aprx = TRUE, n_threads = 4L, fix = length(beta) + 1L,
    # we changed these parameters
    maxvls = 25000L, rel_eps = 1e-3, minvls = 5000L)
  
  # report to console and return
  message(sprintf("\nLog likelihood %.5f (%.5f). Estimated parameters:", 
                  -opt_out$value, -opt_out_quick$value))
  message(paste0(capture.output(print(
    c(opt_out$par, Scale = sig))), collapse = "\n"))
  
  list(opt_out_quick = opt_out_quick, opt_out = opt_out)
})
```

We can construct an approximate 95% confidence interval using an estimated 
cubic smoothing spline for the profile likelihood (more `sigs` points may be 
needed to get a good estimate of the smoothing spline):

```{r conf_int_simple_ex}
# get the critical values
alpha <- .05
crit_val <- qchisq(1 - alpha, 1)

# fit the cubic smoothing spline
pls <- -sapply(pl_curve_res, function(x) x$opt_out$value)
smooth_est <- smooth.spline(sigs, pls)

# check that we have values within the bounds
max_ml <- -opt_out$value
ll_diffs <- 2 * (max_ml - pls)
stopifnot(any(head(ll_diffs, length(ll_diffs) / 2) > crit_val), 
          any(tail(ll_diffs, length(ll_diffs) / 2) > crit_val))

# find the values
max_par <- tail(opt_out$par, 1)
lb <- uniroot(function(x) 2 * (max_ml - predict(smooth_est, x)$y) - crit_val, 
              c(min(sigs)       , exp(max_par / 2)))$root
ub <- uniroot(function(x) 2 * (max_ml - predict(smooth_est, x)$y) - crit_val, 
              c(exp(max_par / 2), max(sigs)))$root

# the confidence interval 
c(lb, ub)
c(lb, ub)^2 # on the variance scale
```

A caveat is that issues with the $\chi^2$ approximation may arise on the 
boundary of the scale parameter ($\sigma = 0$; e.g. 
see https://stats.stackexchange.com/a/4894/81865).
Notice that the above may fail if the estimated profile likelihood is not 
smooth e.g. because of convergence issues. We can plot the profile likelihood 
and highlight the critical value as follows:

```{r plot_simple_ex_profile_likelihood}
par(mar = c(5, 5, 1, 1))
plot(sigs, pls, bty = "l",
     pch = 16, xlab = expression(sigma), ylab = "Profile likelihood")
grid()
lines(predict(smooth_est, seq(min(sigs), max(sigs), length.out = 100)))
abline(v = exp(tail(opt_out$par, 1) / 2), lty = 2) # the estimate
abline(v = sqrt(attr(dat, "sig_sq")), lty = 3) # the true value
abline(v = lb, lty = 3) # mark the lower bound
abline(v = ub, lty = 3) # mark the upper bound
abline(h = max_ml - crit_val / 2, lty = 3) # mark the critical value
```

The `pedmod_profile` function is a convenience function to do like above. An 
example of using the `pedmod_profile` function is provided below:

```{r pre_simple_pedmod_profile, echo = FALSE}
ll_terms <- pedigree_ll_terms(dat, max_threads = 4L)
```

```{r simple_pedmod_profile, cache = 1, message = TRUE, fig.keep="last"}
# find the profile likelihood based confidence interval
prof_res <- pedmod_profile(
  ptr = ll_terms, par = opt_out$par, delta = .5, maxvls = 10000L, 
  minvls = 1000L, alpha = .05, abs_eps = 0, rel_eps = 1e-4, which_prof = 4L,
  use_aprx = TRUE, n_threads = 4L, verbose = TRUE)

# the confidence interval for the scale parameter
exp(prof_res$confs)

# compare with Wald based confidence intervals on the log scale
Wald_conf <- tail(opt_out$par, 1) + c(-1, 1) * qnorm(.975) *
  sqrt(tail(diag(attr(hess, "vcov")), 1))
rbind(Wald = Wald_conf, `Profile likelihood` = prof_res$confs)

# plot the estimated profile likelihood curve and check that everything looks 
# fine
sigs <- exp(prof_res$xs / 2)
pls <- prof_res$p_log_Lik
par(mar = c(5, 5, 1, 1))
plot(log(sigs), pls, bty = "l",
     pch = 16, xlab = expression(log(sigma)), ylab = "Profile likelihood")
grid()
smooth_est <- smooth.spline(log(sigs), pls)
lines(predict(smooth_est, log(seq(min(sigs), max(sigs), length.out = 100))))
abline(v = exp(tail(opt_out$par, 1) / 2), lty = 2) # the estimate
abline(v = sqrt(attr(dat, "sig_sq")), lty = 3) # the true value
abline(h = max(pls) - qchisq(.95, 1) / 2, lty = 3) # mark the critical value

abline(v = Wald_conf / 2, lty = 4) # Wald
abline(v = prof_res$confs / 2, lty = 3) # Profile likelihood

# we can do the same for the slope of the binary covariates
prof_res <- pedmod_profile(
  ptr = ll_terms, par = opt_out$par, delta = .5, maxvls = 10000L, 
  minvls = 1000L, alpha = .05, abs_eps = 0, rel_eps = 1e-4, which_prof = 3L,
  use_aprx = TRUE, n_threads = 4L, verbose = TRUE)

# the confidence interval for the slope of the binary covariate
prof_res$confs

# compare w/ Wald
Wald_conf <- opt_out$par[3] + c(-1, 1) * qnorm(.975) *
  sqrt(diag(attr(hess, "vcov"))[3])
rbind(Wald = Wald_conf, `Profile likelihood` = prof_res$confs)
```

```{r binary_simple_pedmod_profile, cache = 1, message = TRUE, fig.keep="last"}
# plot the estimated profile likelihood curve and check that everything looks 
# fine
bin_slope <- prof_res$xs
pls <- prof_res$p_log_Lik
par(mar = c(5, 5, 1, 1))
plot(bin_slope, pls, bty = "l",
     pch = 16, xlab = expression(beta[2]), ylab = "Profile likelihood")
grid()
lines(spline(bin_slope, pls, n = 100))
abline(v = opt_out$par[3], lty = 2) # the estimate
abline(v = attr(dat, "beta")[3], lty = 3) # the true value
abline(h = max(pls) - qchisq(.95, 1) / 2, lty = 3) # mark the critical value
```

We only ran the above with one seed. We can draw the curve with using different
seeds to check if this does not change the estimates.  We will likely need to 
use more samples if the result depends on the seed.

```{r simple_ex_more_profile_likelihood, cache = 1}
# compute the profile likelihood using different seeds
pl_curve_res <- lapply(1:5, function(seed) pedmod_profile(
  ptr = ll_terms, par = opt_out$par, delta = .5, maxvls = 10000L, 
  minvls = 1000L, alpha = .05, abs_eps = 0, rel_eps = 1e-4, which_prof = 4L,
  use_aprx = TRUE, n_threads = 4L, seed = seed))
```

We show the estimated profile likelihood based confidence intervals below:

```{r draw_mult_pf_curves}
# the profile likelihood based confidence intervals
print(exp(t(sapply(pl_curve_res, `[[`, "confs"))), digits = 8)
```

### Randomized Quasi-Monte Carlo
There are two randomized quasi-Monte Carlo methods which are implemented in the 
package: randomized Korobov rules as in the implementation by @Genz02 and 
scrambled Sobol sequences. The former is used by default. The questions is which 
method to use. As an example, we will increase the number of samples with either
methods and see how this effects the error for the gradient of the log 
likelihood from the first couple of families. We do this below:

```{r pre_rqmc, echo=FALSE}
ll_terms <- pedigree_ll_terms(dat, max_threads = 4L)
```

```{r rqmc, cache = 1}
# create a simple function which computes the gradient. We set the convergence 
# threshold values low such that all the samples will be used
gr <- function(maxvls, method, par = start$par, minvls = 500L)
  eval_pedigree_grad(ptr = ll_terms, par = par, maxvls = maxvls, abs_eps = 0,
                     rel_eps = 1e-12, indices = 0:9, minvls = minvls, 
                     method = method, n_threads = 4L)

# compute the estimator for either method using an increasing number of samples
n_samp <- 1000 * 2^(0:9) # the sample sizes we will use
seeds <- 1:40 # the seeds we will use

res <- sapply(setNames(n_samp, n_samp), function(maxvls){
  sapply(c(Korobov = 0, Sobol = 1), function(method){
    # estimate the gradient
    ests <- sapply(seeds, function(s){
      set.seed(s)
      gr(maxvls = maxvls, minvls = maxvls, method = method)
    })
    
    # return the mean of the estimators and the standard deviation
    rbind(mean = rowMeans(ests), 
          sd = apply(ests, 1L, sd))
  }, simplify = "array")
}, simplify = "array")

# set the names of the dimensions
dimnames(res) <- list(
  metric = dimnames(res)[[1L]], parameter = names(opt_out$par),
  method = dimnames(res)[[3L]], samples = n_samp)

# they seem to converge to the same estimate as expected
print(t(res["mean", , "Korobov", ]), digits = 6)
print(t(res["mean", , "Sobol"  , ]), digits = 6)

# get a best estimator of the gradient by combining the two
precise_est <- rowMeans(res["mean", , , length(n_samp)])
  
# the standard deviation of the result scaled by the absolute value of the 
# estimated gradient to get the number of significant digits
round(t(res["sd", , "Korobov", ] / abs(precise_est)), 6)
round(t(res["sd", , "Sobol"  , ] / abs(precise_est)), 6)
```

```{r show_res_rqmc}
# look at a log-log regression to check convergence rate. We expect a rate 
# between 0.5, O(sqrt(n)) rate, and 1, O(n) rate, which can be seen from minus  
# the slopes below
coef(lm(t(log(res["sd", , "Korobov", ])) ~ log(n_samp)))
coef(lm(t(log(res["sd", , "Sobol", ])) ~ log(n_samp)))

# plot the two standard deviation estimates
par(mar = c(5, 5, 1, 1))
matplot(n_samp, t(res["sd", , "Korobov", ]), log = "xy", ylab = "L2 error", 
        type = "p", pch = c(0:2, 5L), col = "black", bty = "l", 
        xlab = "Number of samples", ylim = range(res["sd", , , ]))
matlines(n_samp, t(res["sd", , "Korobov", ]), col = "black", lty = 2)

# add the points from Sobol method
matplot(n_samp, t(res["sd", , "Sobol", ]), type = "p", pch = 15:18, 
        col = "darkgray", add = TRUE)
matlines(n_samp, t(res["sd", , "Sobol", ]), col = "darkgray", lty = 3)
```

The above seems to suggest that the randomized Korobov rules are preferable and 
that both method achieve close to a
$O(n^{-1 + \epsilon})$ rate for some small $\epsilon$. Notice that we have to 
set `minvls` equal to `maxvls` to achieve the $O(n^{-1 + \epsilon})$ rate with 
randomized Korobov rules.

We can also consider the convergence rate for the log likelihood. This time, 
we also consider the error using the minimax tilted version suggested by 
@Botev17. We also show 
how the error can be reduced by using fewer randomized qausi-Monte Carlo 
sequences at the cost of the precision of the error estimate:

```{r rqmc_likelihood, cache = 1}
# create a simple function which computes the log likelihood. We set the 
# convergence threshold values low such that all the samples will be used
fn <- function(maxvls, method, par = start$par, ptr = ll_terms,  minvls = 500L, 
               use_tilting)
  eval_pedigree_ll(ptr = ptr, par = par, maxvls = maxvls, abs_eps = 0,
                   rel_eps = 1e-12, indices = 0:9, minvls = minvls, 
                   method = method, n_threads = 4L, use_tilting = use_tilting)

# compute the estimator for either method using an increasing number of samples
res <- sapply(setNames(n_samp, n_samp), function(maxvls){
  sapply(c(`W/ tilting` = TRUE, `W/o tilting` = FALSE), function(use_tilting){
    sapply(c(Korobov = 0, Sobol = 1), function(method){
      # estimate the gradient
      ests <- sapply(seeds, function(s){
        set.seed(s)
        fn(maxvls = maxvls, minvls = maxvls, method = method, 
           use_tilting = use_tilting)
      })
      
      # return the mean of the estimators and the standard deviation
      c(mean = mean(ests), sd = sd(ests))
    }, simplify = "array")
  }, simplify  = "array")
}, simplify = "array")

# compute the errors with fewer randomized quasi-Monte Carlo sequences
ll_terms_few_sequences <- pedigree_ll_terms(dat, max_threads = 4L, 
                                            n_sequences = 1L)
res_few_seqs <- sapply(setNames(n_samp, n_samp), function(maxvls){
  sapply(c(`W/ tilting` = TRUE, `W/o tilting` = FALSE), function(use_tilting){
    sapply(c(Korobov = 0, Sobol = 1), function(method){
      # estimate the gradient
      ests <- sapply(seeds, function(s){
        set.seed(s)
        fn(maxvls = maxvls, minvls = maxvls, method = method, 
           ptr = ll_terms_few_sequences, use_tilting = use_tilting)
      })
      
      # return the mean of the estimators and the standard deviation
      c(mean = mean(ests), sd = sd(ests))
    }, simplify = "array")
  }, simplify = "array")
}, simplify = "array")
```

```{r show_rqmc_likelihood}
# the standard deviation of the result scaled by the absolute value of the 
# estimated log likelihood to get the number of significant digits. Notice that
# we scale up the figures by 1000!
precise_est <- mean(res["mean", , , length(n_samp)])
round(1000 * res["sd", "Korobov", , ] / abs(precise_est), 6)
round(1000 * res["sd", "Sobol"  , , ] / abs(precise_est), 6)

# with fewer sequences
round(1000 * res_few_seqs["sd", "Korobov", , ] / abs(precise_est), 6)
round(1000 * res_few_seqs["sd", "Sobol"  , , ] / abs(precise_est), 6)

# look at log-log regressions
apply(res["sd", , , ], 1:2, function(sds) coef(lm(log(sds) ~ log(n_samp))))

# plot the standard deviation estimates. Dashed lines are with fewer sequences
par(mar = c(5, 5, 1, 1))
create_plot <- function(results, ylim){
  sds <- matrix(results["sd", , , ], ncol = dim(results)[4])
  dimnames(sds) <- 
    list(do.call(outer, c(dimnames(results)[2:3], list(FUN = paste))), NULL)
  
  lty <- c(1, 1, 2, 2)
  col <- rep(c("black", "darkgray"), 2)
  matplot(n_samp, t(sds), log = "xy", ylab = "L2 error", lty = lty, 
        type = "l", bty = "l", xlab = "Number of samples", 
        col = col, ylim = ylim)
  matplot(n_samp, t(sds), pch = c(1, 16), col = col, 
          add = TRUE)
  legend("bottomleft", bty = "n", lty = lty, col = col, 
         legend = rownames(sds))
  grid()
}

# with more sequences
ylim_plot <- range(res["sd", , , ], res_few_seqs["sd", , , ])
create_plot(res, ylim = ylim_plot)
# with one sequence
create_plot(res_few_seqs, ylim = ylim_plot)
```

Again the randomized Korobov rules seems preferable. 
In general, a strategy can be to use only one 
randomized quasi-Monte Carlo sequence as above and set `minvls` and `maxvls` 
to the desired number of samples. This will though imply that 
the method cannot stop 
early if it is easy to approximate the log likelihood and its derivative. 
We fit the model again below as example of using the scrambled Sobol sequences:

```{r rqmc_est_simple, cache = 1}
# estimate the model using Sobol sequences
system.time(
  opt_out_sobol <- pedmod_opt(
    ptr = ll_terms, par = start$par, abs_eps = 0, use_aprx = TRUE, 
    n_threads = 4L, 
    maxvls = 25000L, rel_eps = 1e-3, minvls = 5000L, method = 1L))

# compare the result. We start with the log likelihood
print(-opt_out_sobol$value, digits = 8)
print(-opt_out      $value, digits = 8)

# the parameters
rbind(Korobov = opt_out      $par, 
      Sobol   = opt_out_sobol$par)

# number of used function and gradient evaluations
opt_out$counts
opt_out_sobol$counts
```

### Simulation Study

We make a small simulation study below where we are interested in the estimation 
time, bias and coverage of Wald type confidence intervals.

```{r clean_pre_sim_study_simple, echo = FALSE}
rm(list = setdiff(ls(), c("sim_dat", "ped", "fam", "std_prop_estimates")))
```

```{r sim_study_simple}
# the seeds we will use
seeds <- c(36451989L, 18774630L, 76585289L, 31898455L, 55733878L, 99681114L, 37725150L, 99188448L, 66989159L, 20673587L, 47985954L, 42571905L, 53089211L, 18457743L, 96049437L, 70222325L, 86393368L, 45380572L, 81116968L, 48291155L, 89755299L, 69891073L, 1846862L, 15263013L, 37537710L, 
           25194071L, 14471551L, 38278606L, 55596031L, 5436537L, 75008107L, 83382936L, 50689482L, 71708788L, 52258337L, 23423931L, 61069524L, 24452554L, 32406673L, 14900280L, 24818537L, 59733700L, 82407492L, 95500692L, 62528680L, 88728797L, 9891891L, 36354594L, 69630736L, 41755287L)

# run the simulation study
sim_study <- lapply(seeds, function(s){
  set.seed(s)
  
  # only run the result if it has not been computed
  f <- file.path("cache", "sim_study_simple", paste0("simple-", s, ".RDS"))
  if(!file.exists(f)){
    # simulate the data
    dat <- sim_dat(n_fams = 400L)
    
    # get the starting values
    library(pedmod)
    do_fit <- function(standardized){
      ll_terms <- pedigree_ll_terms(dat, max_threads = 4L)
      ti_start <- system.time(start <- pedmod_start(
        ptr = ll_terms, data = dat, n_threads = 4L, 
        standardized = standardized))
      start$time <- ti_start
      
      ti_fit <- system.time(
        opt_out <- pedmod_opt(
          ptr = ll_terms, par = start$par, abs_eps = 0, use_aprx = TRUE, 
          n_threads = 4L, 
          maxvls = 25000L, rel_eps = 1e-3, minvls = 5000L, 
          standardized = standardized))
      opt_out$time <- ti_fit
      
      if(standardized){
        start$par   <- standardized_to_direct(start$par, 1L)
        opt_out$par <- standardized_to_direct(opt_out$par, 1L)
      }
      
      if(!standardized){
        hess_time <- system.time(
          hess <- eval_pedigree_hess(
            ptr = ll_terms, par = opt_out$par, maxvls = 25000L, 
            abs_eps = 0, minvls = 5000L, use_aprx = TRUE, 
            rel_eps = 1e-4, n_threads = 4L))
        attr(hess, "time") <- hess_time
      } else
        hess <- NULL
      
      list(start = start, opt_out = opt_out, hess = hess,
           ll_no_rng = start$logLik_no_rng)
    }
    
    fit_direct <- do_fit(standardized = FALSE)
    fit_std    <- do_fit(standardized = TRUE)
    saveRDS(list(fit_direct = fit_direct, fit_std = fit_std), f)
  }
  
  # report to console and return 
  out <- readRDS(f)
  message(paste0(capture.output(out$fit_direct$opt_out$par), collapse = "\n"))
  message(paste0(capture.output(out$fit_std   $opt_out$par), collapse = "\n"))
  
  par <- out$fit_direct$opt_out$par
  SEs <- sqrt(diag(attr(out$fit_direct$hess, "vcov")))
  
  message(paste0(capture.output(rbind(
    Estimate = par, SE = SEs)), collapse = "\n"))
  message(sprintf(
    "Time %12.1f, %12.1f. Max ll: %12.4f, %12.4f\n",
    with(out$fit_direct, start$time["elapsed"] + opt_out$time["elapsed"]),
    with(out$fit_std   , start$time["elapsed"] + opt_out$time["elapsed"]),
    -out$fit_direct$opt_out$value,
    -out$fit_std   $opt_out$value))
  
  out
})

# gather the estimates
beta_est <- sapply(sim_study, function(x) 
  cbind(Direct       = head(x$fit_direct$opt_out$par, 3), 
        Standardized = head(x$fit_std   $opt_out$par, 3)), 
  simplify = "array")
sigma_est <- sapply(sim_study, function(x) 
  cbind(Direct       = exp(tail(x$fit_direct$opt_out$par, 1) / 2), 
        Standardized = exp(tail(x$fit_std   $opt_out$par, 1) / 2)), 
  simplify = "array")

# compute the errors
tmp <- sim_dat(2L)
err_beta  <- beta_est  - attr(tmp, "beta")
err_sigma <- sigma_est - sqrt(attr(tmp, "sig_sq"))
dimnames(err_sigma)[[1L]] <- "std genetic"
err <- abind::abind(err_beta, err_sigma, along = 1)

# get the bias estimates and the standard errors
bias <- apply(err, 1:2, mean)
n_sims <- dim(err)[[3]]
SE <- apply(err , 1:2, sd) / sqrt(n_sims)
bias
SE

# make a box plot
b_vals <- expand.grid(rownames(err), strtrim(colnames(err), 1))
box_dat <- data.frame(Error = c(err), 
                      Parameter = rep(b_vals$Var1, n_sims), 
                      Method = rep(b_vals$Var2, dim(err)[[3]]))
par(mar = c(7, 5, 1, 1))
# S is for the standardized and D is for the direct parameterization
boxplot(Error ~ Method + Parameter, box_dat, ylab = "Error", las = 2, 
        xlab = "")
abline(h = 0, lty = 2)
grid()
# get the average computation times
time_vals <- sapply(sim_study, function(x) {
  . <- function(z){
    keep <- c("opt_out", "start")
    out <- setNames(sapply(z[keep], function(z) z$time["elapsed"]), keep)
    c(out, total = sum(out))
  }
  
  rbind(Direct       = .(x$fit_direct), 
        Standardized = .(x$fit_std))
}, simplify = "array")
apply(time_vals, 1:2, mean)
apply(time_vals, 1:2, sd)
apply(time_vals, 1:2, quantile)

# get the standardized errors
ers_sds <- sapply(sim_study, function(x){
  par <- x$fit_direct$opt_out$par
  err <- par - c(attr(tmp, "beta"), log(attr(tmp, "sig_sq")))
  SEs <- sqrt(diag(attr(x$fit_direct$hess, "vcov")))
  err / SEs
})

rowMeans(abs(ers_sds) < qnorm(.95)) # 90% coverage
rowMeans(abs(ers_sds) < qnorm(.975)) # 95% coverage
rowMeans(abs(ers_sds) < qnorm(.995)) # 99% coverage

# stats for the computation time of the Hessian
hess_time <- sapply(
  sim_study, function(x) attr(x$fit_direct$hess, "time")["elapsed"])
mean(hess_time)
quantile(hess_time, probs = seq(0, 1, .1))

# compute the coverage on the standardized scale with the proportion of 
# variances
ers_sds <- sapply(sim_study, function(x){
  par_n_vcov <- std_prop_estimates(
    x$fit_direct$opt_out$par, 1L, x$fit_direct$hess)
  truth <- std_prop_estimates(c(attr(tmp, "beta"), log(attr(tmp, "sig_sq"))), 1)
  (par_n_vcov$par - truth$par) / sqrt(diag(par_n_vcov$vcov_var))
})

rowMeans(abs(ers_sds) < qnorm(.95)) # 90% coverage
rowMeans(abs(ers_sds) < qnorm(.975)) # 95% coverage
rowMeans(abs(ers_sds) < qnorm(.995)) # 99% coverage
```

## Example: Adding Child Environment Effects 

```{r clean_pre_simple_w_ev, echo = FALSE}
rm(list = setdiff(ls(), c("ped", "fam", "std_prop_estimates")))
```

As an extension, we can add a child environment effect. The new scale matrix, 
the $C_{i2}$'s, can be written as:

```{r xtra_mat_simple_w_ev}
C_env <- diag(1, NROW(fam))
C_env[c(3, 5), c(3, 5)] <- 1
C_env[c(7:8 ), c(7:8 )] <- 1
C_env[c(9:10), c(9:10)] <- 1

Matrix::Matrix(C_env, sparse = TRUE)
```

We assign the new simulation function below but this time we include only 
binary covariates:

```{r simple_w_ev_assign_sim_dat}
# simulates a data set. 
# 
# Args:
#   n_fams: number of families.
#   beta: the fixed effect coefficients.
#   sig_sq: the scale parameters.
sim_dat <- function(n_fams, beta = c(-3, 4), sig_sq = c(2, 1)){
  # setup before the simulations
  Cmat <- 2 * kinship(ped)
  n_obs <- NROW(fam)
  Sig <- diag(n_obs) + sig_sq[1] * Cmat + sig_sq[2] * C_env
  Sig_chol <- chol(Sig)
  
  # simulate the data
  out <- replicate(
    n_fams, {
      # simulate covariates
      X <- cbind(`(Intercept)` = 1, Binary = runif(n_obs) > .9)
      
      # assign the linear predictor + noise
      eta <- drop(X %*% beta) + drop(rnorm(n_obs) %*% Sig_chol)
      
      # return the list in the format needed for the package
      list(y = as.numeric(eta > 0), X = X, scale_mats = list(
        Genetic = Cmat, Environment = C_env))
    }, simplify = FALSE)
  
  # add attributes with the true values and return 
  attributes(out) <- list(beta = beta, sig_sq = sig_sq)
  out
}
```

The model is 

$$\begin{align*}
 Y_{ij} &= \begin{cases} 1 & \beta_0 + \beta_1 B_{ij} + E_{ij} + G_{ij} + R_{ij} > 0 \\ 0 & \text{otherwise} \end{cases} \\
 X_{ij} &\sim N(0, 1) \\
 B_{ij} &\sim \text{Bin}(0.1, 1) \\
 (G_{i1}, \dots, G_{in_{i}})^\top &\sim N^{(n_i)}(\vec 0, \sigma^2_G C_{i1}) \\
(E_{i1}, \dots, E_{in_{i}})^\top &\sim N^{(n_i)}(\vec 0, \sigma^2_E C_{i2}) \\
 R_{ij} &\sim N(0, 1)\end{align*}$$
 
where $C_{i1}$ is two times the kinship matrix, $C_{i2}$ is singular matrix
for the environment effect, and 
$B_{ij}$ is an observed covariate.
In this case, we exploit that some of log marginal likelihood terms are 
identical. That is, some of the combinations of pedigrees, covariates, and 
outcomes match. Therefor, we can use the `cluster_weights` arguments to reduce 
the computation time as shown below:

```{r simple_w_ev_sim, cache = 1}
# simulate a data set
set.seed(27107390)
dat <- sim_dat(n_fams = 1000L)

# compute the log marginal likelihood by not using that some of the log marginal 
# likelihood terms are identical
beta_true   <- attr(dat, "beta")
sig_sq_true <- attr(dat, "sig_sq")

library(pedmod)
ll_terms_wo_weights <- pedigree_ll_terms(dat, max_threads = 4L)
system.time(ll_res <- eval_pedigree_ll(
  ll_terms_wo_weights, c(beta_true, log(sig_sq_true)), maxvls = 100000L, 
  abs_eps = 0, rel_eps = 1e-3, minvls = 2500L, use_aprx = TRUE, n_threads = 4))
system.time(grad_res <- eval_pedigree_grad(
  ll_terms_wo_weights, c(beta_true, log(sig_sq_true)), maxvls = 100000L, 
  abs_eps = 0, rel_eps = 1e-3, minvls = 2500L, use_aprx = TRUE, n_threads = 4))

# find the duplicated combinations of pedigrees, covariates, and outcomes. One 
# likely needs to change this code if the pedigrees are not identical but are 
# identical if they are permuted. In this case, the code below will miss 
# identical log marginal likelihood terms
dat_unqiue <- dat[!duplicated(dat)]
attributes(dat_unqiue) <- attributes(dat)
length(dat_unqiue) # number of unique terms

# get the weights. This can be written in a much more efficient way
c_weights <- sapply(dat_unqiue, function(x)
  sum(sapply(dat, identical, y = x)))

# get the C++ object and show that the computation time is reduced
ll_terms <- pedigree_ll_terms(dat_unqiue, max_threads = 4L)

system.time(ll_res_fast <- eval_pedigree_ll(
  ll_terms, c(beta_true, log(sig_sq_true)), maxvls = 100000L, abs_eps = 0, 
  rel_eps = 1e-3, minvls = 2500L, use_aprx = TRUE, n_threads = 4, 
  cluster_weights = c_weights))
system.time(grad_res_fast <- eval_pedigree_grad(
  ll_terms, c(beta_true, log(sig_sq_true)), maxvls = 100000L, abs_eps = 0, 
  rel_eps = 1e-3, minvls = 2500L, use_aprx = TRUE, n_threads = 4, 
  cluster_weights = c_weights))

# show that we get the same (up to a Monte Carlo error)
print(c(redundant = ll_res, fast = ll_res_fast), digits = 6)
rbind(redundant = grad_res, fast = grad_res_fast)
rm(dat) # will not need this anymore

# note that the variance is greater for the weighted version
ll_ests <- sapply(1:50, function(seed){
  set.seed(seed)
  eval_pedigree_ll(
    ll_terms_wo_weights, c(beta_true, log(sig_sq_true)), maxvls = 100000L, 
    abs_eps = 0, rel_eps = 1e-3, minvls = 2500L, use_aprx = TRUE, n_threads = 4)
})
ll_ests_fast <- sapply(1:50, function(seed){
  set.seed(seed)
  eval_pedigree_ll(
    ll_terms, c(beta_true, log(sig_sq_true)), maxvls = 10000L, abs_eps = 0, 
    rel_eps = 1e-3, minvls = 2500L, use_aprx = TRUE, n_threads = 4, 
    cluster_weights = c_weights)
})

# the estimates are comparable
c(`Without weights` = mean(ll_ests), `With weights` = mean(ll_ests_fast))

# the standard deviation is different
c(`Without weights` = sd(ll_ests), `With weights` = sd(ll_ests_fast))

# we can mitigate this by using the vls_scales argument which though is a bit 
# slower
ll_ests_fast_vls_scales <- sapply(1:50, function(seed){
  set.seed(seed)
  eval_pedigree_ll(
    ll_terms, c(beta_true, log(sig_sq_true)), maxvls = 10000L, abs_eps = 0, 
    rel_eps = 1e-3, minvls = 2500L, use_aprx = TRUE, n_threads = 4, 
    cluster_weights = c_weights, vls_scales = sqrt(c_weights))
})

# the estimates are comparable
c(`Without weights` = mean(ll_ests), `With weights` = mean(ll_ests_fast), 
  `With weights and vls_scales` = mean(ll_ests_fast_vls_scales))

# the standard deviation is different
c(`Without weights` = sd(ll_ests), `With weights` = sd(ll_ests_fast), 
  `With weights and vls_scales` = sd(ll_ests_fast_vls_scales))

# it is still faster
system.time(ll_res_fast <- eval_pedigree_ll(
  ll_terms, c(beta_true, log(sig_sq_true)), maxvls = 100000L, abs_eps = 0, 
  rel_eps = 1e-3, minvls = 2500L, use_aprx = TRUE, n_threads = 4, 
  cluster_weights = c_weights, vls_scales = sqrt(c_weights)))
system.time(grad_res_fast <- eval_pedigree_grad(
  ll_terms, c(beta_true, log(sig_sq_true)), maxvls = 100000L, abs_eps = 0, 
  rel_eps = 1e-3, minvls = 2500L, use_aprx = TRUE, n_threads = 4, 
  cluster_weights = c_weights, vls_scales = sqrt(c_weights)))

# find the starting values
system.time(start <- pedmod_start(
  ptr = ll_terms, data = dat_unqiue, cluster_weights = c_weights, 
  vls_scales = sqrt(c_weights)))

# optimize
system.time(
  opt_out_quick <- pedmod_opt(
    ptr = ll_terms, par = start$par, abs_eps = 0, use_aprx = TRUE, 
    n_threads = 4L,  cluster_weights = c_weights,
    maxvls = 5000L, rel_eps = 1e-2, minvls = 500L, 
    vls_scales = sqrt(c_weights)))
system.time(
  opt_out <- pedmod_opt(
    ptr = ll_terms, par = opt_out_quick$par, abs_eps = 0, use_aprx = TRUE, 
    n_threads = 4L,  cluster_weights = c_weights, vls_scales = sqrt(c_weights),
    # we changed these parameters
    maxvls = 25000L, rel_eps = 1e-3, minvls = 5000L))
```

The results are shown below:

```{r est_simple_w_res}
# parameter estimates versus the truth
rbind(opt_out       = head(opt_out$par, -2), 
      opt_out_quick = head(start  $par, -2), 
      truth         = attr(dat_unqiue, "beta"))
rbind(opt_out       = exp(tail(opt_out$par, 2)), 
      opt_out_quick = exp(tail(start  $par, 2)), 
      truth         = attr(dat_unqiue, "sig_sq"))

# log marginal likelihoods
print( start  $logLik_est, digits = 8)  # this is unreliably/imprecise
print(-opt_out$value     , digits = 8)
```

We compute the Hessian like before to get the standard errors.

```{r hess_est_simple_w_emv, cache = 1}
set.seed(1)
system.time(hess <- eval_pedigree_hess(
  ptr = ll_terms, par = opt_out$par, maxvls = 25000L, minvls = 5000L, abs_eps = 0, 
  rel_eps = 1e-4, do_reorder = TRUE, use_aprx = FALSE, n_threads = 4L,
  cluster_weights = c_weights, vls_scales = sqrt(c_weights)))

# the gradient is quite small
sqrt(sum(attr(hess, "grad")^2))

# show parameter estimates along with standard errors
rbind(Estimates = opt_out$par, 
      SE = sqrt(diag(attr(hess, "vcov"))))
rbind(Estimates = c(head(opt_out$par, -2), exp(tail(opt_out$par, 2))), 
      SE = sqrt(diag(attr(hess, "vcov_org"))))
```

Again, we can look at the estimates with the standardized fixed effects 
coefficients and the proportion of variances.

```{r env_std_coef_n_prop_var}
# show the transformed estimates along with standard errors
std_prop <- std_prop_estimates(opt_out$par, n_scales = 2L, hess = hess)
rbind(
  Truth = std_prop_estimates(
    c(attr(dat_unqiue, "beta"), log(attr(dat_unqiue, "sig_sq"))), 2)$par,
  Estimates = std_prop$par, SE = sqrt(diag(std_prop$vcov_var)))
```

### Motivation of Different Number of Samples

We use the `cluster_weights` argument above to exploit that some of the 
log marginal likelihood terms are identical. Specifically, let $l_j$ be the 
$j$th distinct log marginal likelihood term 
and $\vec\theta$ be the model parameters, then we use 
that the log marginal likelihood is 

$$l(\vec\theta) = \sum_{j = 1}^L\sum_{i = 1}^{w_j}l_j(\vec\theta) = \sum_{j = 1}^Lw_jl_j(\vec\theta).$$

The unweighted version is the left hand side and the weighted version is the 
right hand side. The two have different variances. Our 
quasi-Monte-Carlo method has (almost) a variance for each $\exp l_j$ which is 
$\mathcal{O}(m^{-2})$ with $m$ being the number of samples we use for each 
$l_j$. Thus, the variance of the unweighted version is

$$\sum_{l = j}^L\sum_{i = 1}^{w_j}\text{Var}(l_j(\vec\theta))$$

which is 

$$\mathcal{O}\left(\sum_{j = 1}^L \frac{w_j}{m^2}\right)$$

However, the variance of the weighted version is

$$\sum_{j = 1}^L\text{Var}(w_jl_j(\vec\theta))$$

which is 

$$\mathcal{O}\left(\sum_{j = 1}^L \frac{w_j^2}{m^2}\right)$$

Though, we can get a similar variance by using $\sqrt{w_j}m$ samples for 
term $j$. The variance then becomes

$$\mathcal{O}\left(\sum_{j = 1}^L \frac{w_j^2}{w_jm^2}\right) = \mathcal{O}\left(\sum_{j = 1}^L \frac{w_j}{m^2}\right)$$
but we do so using only 

$$m\sum_{j = 1}^L\sqrt{w_j}$$

samples rather than 

$$m\sum_{j = 1}^Lw_j.$$

### Alternative Parameterization

As before, we can also work with the standardized parameterization. 

```{r std_simple_w_ev_sim, cache = 1}
#####
# transform the parameters and check that we get the same likelihood
std_par <- direct_to_standardized(opt_out$par, n_scales = 2L)
std_par # the standardized parameterization
opt_out$par # the direct parameterization 

# we can map back as follows
par_back <- standardized_to_direct(std_par, n_scales = 2L)
all.equal(opt_out$par, par_back, check.attributes = FALSE)
# the proportion of variance of each effect
attr(par_back, "variance proportions") 

# the proportions match
total_var <- sum(exp(tail(opt_out$par, 2))) + 1
exp(tail(opt_out$par, 2)) / total_var

# compute the likelihood with either parameterization
set.seed(1L)
eval_pedigree_ll(ptr = ll_terms, par = opt_out$par, maxvls = 10000L, 
                 minvls = 1000L, rel_eps = 1e-3, use_aprx = TRUE, abs_eps = 0, 
                 cluster_weights = c_weights, vls_scales = sqrt(c_weights))
set.seed(1L)
eval_pedigree_ll(ptr = ll_terms, par = std_par    , maxvls = 10000L, 
                 minvls = 1000L, rel_eps = 1e-3, use_aprx = TRUE, abs_eps = 0, 
                 cluster_weights = c_weights, vls_scales = sqrt(c_weights),
                 standardized = TRUE)

# we can also get the same gradient with an application of the chain rule
jac <- attr(
  standardized_to_direct(std_par, n_scales = 2L, jacobian = TRUE), 
  "jacobian")

set.seed(1L)
g1 <- eval_pedigree_grad(ptr = ll_terms, par = opt_out$par, maxvls = 10000L, 
                         minvls = 1000L, rel_eps = 1e-3, use_aprx = TRUE, 
                         abs_eps = 0, cluster_weights = c_weights, 
                         vls_scales = sqrt(c_weights))
set.seed(1L)
g2 <- eval_pedigree_grad(ptr = ll_terms, par = std_par, maxvls = 10000L, 
                         minvls = 1000L, rel_eps = 1e-3, use_aprx = TRUE, 
                         abs_eps = 0, standardized = TRUE,  
                         cluster_weights = c_weights, 
                         vls_scales = sqrt(c_weights))
all.equal(drop(g1 %*% jac), g2, check.attributes = FALSE)
```

The model can also be estimated with the the standardized parameterization:

```{r continued_std_simple_w_ev_sim, cache = 1}
# perform the optimization. We start with finding the starting values
system.time(start_std <- pedmod_start(
  ptr = ll_terms, data = dat_unqiue, cluster_weights = c_weights, 
  vls_scales = sqrt(c_weights), standardized = TRUE))

# are the starting values similar?
standardized_to_direct(start_std$par, n_scales = 2L)
start$par

# this may have required different number of gradient and function evaluations
start_std$opt$counts
start    $opt$counts

# estimate the model
system.time(
  opt_out_quick_std <- pedmod_opt(
    ptr = ll_terms, par = start_std$par, abs_eps = 0, use_aprx = TRUE, 
    n_threads = 4L,  cluster_weights = c_weights, standardized = TRUE,
    maxvls = 5000L, rel_eps = 1e-2, minvls = 500L, 
    vls_scales = sqrt(c_weights)))
system.time(
  opt_out_std <- pedmod_opt(
    ptr = ll_terms, par = opt_out_quick_std$par, abs_eps = 0, use_aprx = TRUE, 
    n_threads = 4L,  cluster_weights = c_weights, standardized = TRUE,
    vls_scales = sqrt(c_weights),
    # we changed these parameters
    maxvls = 25000L, rel_eps = 1e-3, minvls = 5000L))

# we get the same
standardized_to_direct(opt_out_std$par, n_scales = 2L)
opt_out$par

# this may have required different number of gradient and function evaluations
opt_out_quick_std$counts
opt_out_quick    $counts

opt_out_std$counts
opt_out    $counts
```

### Profile Likelihood Curve

We can make a 2D profile likelihood curve as follows:

```{r simple_w_ev_ex_profile_likelihood, cache = 1}
# get the values at which we evaluate the profile likelihood
rg <- Map(function(est, truth)
  range(exp(est / 2) * c(.8, 1.25), truth), 
  est = tail(opt_out$par, 2), truth = sqrt(attr(dat_unqiue, "sig_sq")))

sig_vals1 <- seq(rg[[1]][1], rg[[1]][2], length.out = 5)
sig_vals2 <- seq(rg[[2]][1], rg[[2]][2], length.out = 5)
sigs <- expand.grid(sigma1 = sig_vals1,
                    sigma2 = sig_vals2)

# function to compute the profile likelihood. 
# 
# Args:
#   fix: indices of parameters to fix. 
#   fix_val: values of the fixed parameters.
#   sig_start: starting values for the scale parameters.
ll_terms <- pedigree_ll_terms(dat_unqiue, max_threads = 4L)
pl_curve_func <- function(fix, fix_val, 
                          sig_start = exp(tail(opt_out$par, 2) / 2)){
  # get the fixed indices of the fixed parameters
  beta = start$beta_no_rng
  is_fix_beta <- fix <= length(beta)
  fix_beta <- fix[is_fix_beta]
  is_fix_sigs <- fix >  length(beta)
  fix_sigs <- fix[is_fix_sigs]
  
  # set the parameters to pass
  sig <- sig_start
  if(length(fix_sigs) > 0)
    sig[fix_sigs - length(beta)] <- fix_val[is_fix_sigs]
  
  # re-scale beta and setup the sigma argument to pass
  sig_sq_log <- 2 * log(sig)
  beta_scaled <- beta * sqrt(1 + sum(sig^2))
  
  # setup the parameter vector
  fix_par <- c(beta_scaled, sig_sq_log)
  if(length(fix_beta) > 0)
    fix_par[fix_beta] <- fix_val[is_fix_beta]
  
  # optimize like before but using the fix argument
  opt_out_quick <- pedmod_opt(
    ptr = ll_terms, par = fix_par, maxvls = 5000L, abs_eps = 0, 
    rel_eps = 1e-2, minvls = 500L, use_aprx = TRUE, n_threads = 4L, 
    fix = fix, cluster_weights = c_weights, vls_scales = sqrt(c_weights))
  
  # notice that pedmod_opt only returns a subset of the parameters. These are 
  # the parameters that have been optimized over
  par_new <- fix_par
  par_new[-fix] <- opt_out_quick$par
  opt_out <- pedmod_opt(
    ptr = ll_terms, par = par_new, abs_eps = 0, 
    use_aprx = TRUE, n_threads = 4L, fix = fix,
    cluster_weights = c_weights, vls_scales = sqrt(c_weights),
    # we changed these parameters
    maxvls = 25000L, rel_eps = 1e-3, minvls = 5000L)
  
  # report to console and return
  message(sprintf("\nLog likelihood %.5f (%.5f). Estimated parameters:", 
                  -opt_out$value, -opt_out_quick$value))
  message(paste0(capture.output(print(
    c(`non-fixed` = opt_out$par, fixed = fix_par[fix]))), collapse = "\n"))
  
  list(opt_out_quick = opt_out_quick, opt_out = opt_out)
}

# compute the profile likelihood
pl_curve_res <- Map(
  function(sig1, sig2) pl_curve_func(fix = 0:1 + length(opt_out$par) - 1L, 
                                     fix_val = c(sig1, sig2)), 
  sig1 = sigs$sigma1, sig2 = sigs$sigma2)
```

```{r draw_simple_w_ev_ex_profile_likelihood}
par(mfcol = c(2, 2), mar = c(1, 1, 1, 1))
pls <- -sapply(pl_curve_res, function(x) x$opt_out$value)
for(i in 1:3 - 1L)
  persp(sig_vals1, sig_vals2, matrix(pls, length(sig_vals1)), 
        xlab = "\nGenetic", ylab = "\nEnvironment", 
        zlab = "\n\nProfile likelihood", theta = 65 + i * 90, 
        ticktype = "detailed")
```

We may just be interested in creating two profile likelihood curves for each of the 
scale parameters. This can be done as follows:

```{r pre_env_pl_curves, echo = FALSE}
ll_terms <- pedigree_ll_terms(dat_unqiue, max_threads = 4L)
```

```{r env_pl_curves, cache = 1, message=TRUE}
# first we compute data for the two profile likelihood curves staring with the
# curve for the additive genetic effect
pl_genetic <- pedmod_profile(
  ptr = ll_terms, par = opt_out$par, delta = .4, maxvls = 20000L, 
  minvls = 1000L, alpha = .05, abs_eps = 0, rel_eps = 1e-4, which_prof = 3L,
  use_aprx = TRUE, n_threads = 4L, verbose = TRUE, cluster_weights = c_weights, 
  vls_scales = sqrt(c_weights))
exp(pl_genetic$confs) # the confidence interval

# compare with the Wald type
Wald <- 
  opt_out$par[3] + c(-1, 1) * qnorm(.975) * sqrt(diag(attr(hess, "vcov"))[3])
rbind(Wald = Wald, `Profile likelihood` = pl_genetic$confs)

# then we compute the curve for the environment effect
pl_env <- pedmod_profile(
  ptr = ll_terms, par = opt_out$par, delta = .6, maxvls = 20000L, 
  minvls = 1000L, alpha = .05, abs_eps = 0, rel_eps = 1e-4, which_prof = 4L,
  use_aprx = TRUE, n_threads = 4L, verbose = TRUE, cluster_weights = c_weights, 
  vls_scales = sqrt(c_weights))
exp(pl_env$confs) # the confidence interval

# compare with the Wald type
Wald <- 
  opt_out$par[4] + c(-1, 1) * qnorm(.975) * sqrt(diag(attr(hess, "vcov"))[4])
rbind(Wald = Wald, `Profile likelihood` = pl_env$confs)
```

We plot the two profile likelihood curves below:

```{r plot_env_pl_curves}
do_plot <- function(obj, xlab, estimate, trans = function(x) exp(x / 2), 
                    max_diff = 8, add = FALSE, col = "black"){
  xs <- trans(obj$xs)
  pls <- obj$p_log_Lik
  keep <- pls > max(pls) - max_diff
  xs <- xs[keep]
  pls <- pls[keep]
  if(add)
    points(xs, pls, pch = 16, col = col)
  else {
    plot(xs, pls, bty = "l", pch = 16, xlab = xlab, ylab = "Profile likelihood", 
         col = col)
    grid()
    abline(v = estimate, lty = 2, col = col) # the estimate
    # mark the critical value
    abline(h = max(pls) - qchisq(.95, 1) / 2, lty = 3, col = col) 
  }
  
  lines(spline(xs, pls, n = 100L), col = col)
}

par(mar = c(5, 5, 1, 1))
do_plot(pl_genetic, expression(sigma[G]), exp(opt_out$par[3] / 2))
do_plot(pl_env, expression(sigma[E]), exp(opt_out$par[4] / 2))
```

#### Profile Likelihood Curve: Proportion of Variance
Suppose that we want a profile likelihood curve for the proportion of variance 
explained by each random effect. If $K = 1$ then we can use the profile 
likelihood curve for $\sigma_1^2$ as the
proportion of variance for the first effect when $K = 1$ is a monotone 
transformation of this parameter only and thus we can 
use the scale invariance of the likelihood ratio. However, this is not true for 
more effects, $K > 1$. To see this, notice that proportion of variance is given 
by

$$h_i = \left(1 + \sum_{k = 1}^K\sigma_k^2\right)^{-1}\sigma_i^2\Leftrightarrow 
  \sigma_i^2 = 
    \frac{h_i}{1 - h_i}\left(1 + \sum_{k \in \{1,\dots,K\}\setminus\{i\}}\sigma_k^2\right)$$

Let $l(\vec\beta, \sigma_1^2,\dots,\sigma_K^2)$ be the log likelihood. Then the 
profile likelihood in the proportion of variance explained by the $i$th effect 
is 

$$\tilde l_i(h_i) = \max_{\vec\beta,\sigma_1,\dots,\sigma_{k-1},\sigma_{k+1},\dots,\sigma_K}
  l\left(\vec\beta,\sigma_1,\dots,\sigma_{k-1},
  \frac{h_i}{1 - h_i}\left(1 + \sum_{k \in \{1,\dots,K\}\setminus\{i\}}\sigma_k^2\right),
  \sigma_{k+1},\dots,\sigma_K\right)$$

As these proportions are often the interest of the analysis, the 
`pedmod_profile_prop` function is implemented to produce profile likelihood
based confidence intervals for $K > 1$. We provide an example of using 
`pedmod_profile_prop` below. 

```{r pre_prop_var_conf, echo = FALSE}
ll_terms <- pedigree_ll_terms(dat_unqiue, max_threads = 4L)
```

```{r prop_var_conf, cache = 1, message=TRUE}
# confidence interval for the proportion of variance for the genetic effect
pl_genetic_prop <- pedmod_profile_prop(
  ptr = ll_terms, par = opt_out$par, maxvls = 20000L, 
  minvls = 1000L, alpha = .05, abs_eps = 0, rel_eps = 1e-4, which_prof = 1L,
  use_aprx = TRUE, n_threads = 4L, verbose = TRUE, cluster_weights = c_weights, 
  vls_scales = sqrt(c_weights))
pl_genetic_prop$confs # the confidence interval

# confidence interval for the proportion of variance for the environment
# effect
pl_env_prop <- pedmod_profile_prop(
  ptr = ll_terms, par = opt_out$par, maxvls = 20000L, 
  minvls = 1000L, alpha = .05, abs_eps = 0, rel_eps = 1e-4, which_prof = 2L,
  use_aprx = TRUE, n_threads = 4L, verbose = TRUE, cluster_weights = c_weights,
  vls_scales = sqrt(c_weights))
pl_env_prop$confs # the confidence interval
```

A wrong approach is to use the confidence interval for $\sigma_i^2$ to attempt
to construct a confidence interval for $h_i$. To see that this is wrong, let 

$$\begin{align*}
\vec v_{i}(\sigma_i^2) &= 
  \text{arg max}_{\sigma_1^2,\dots,\sigma_{i  -1}^2, \sigma_{i + 1}^2,\dots,\sigma_K^2}
  \max_{\vec\beta}
  l\left(\vec\beta,\sigma_1^2,\dots,\sigma_K^2\right) \\
\vec s_i(\sigma_i^2) &= 
  \left(v_{i1}(\sigma_i^2),\dots,
        v_{i,i-1}(\sigma_i^2), \sigma_i^2, 
        v_{i,i+1}(\sigma_i^2),\dots,
        v_{i,K-1}(\sigma_i^2)\right)^\top
\end{align*}$$
Now, suppose that exists a function $g:\,(0,1)\rightarrow(0,\infty)$ such that

$$h_i = \frac{g_i(h_i)}{1+\sum_{k = 0}^K s_{ik}(g_i(h_i))}$$

Then it follows that 

$$\tilde l_i(h_i) \geq \max_{\vec\beta} l(\vec\beta, \vec s_i(g_i(h_i)))$$

Thus, if one uses the profile likelihood curve of $\sigma_i^2$ to attempt to 
construct a confidence interval for $h_i$ then the result is anti-conservative.
This is illustrated below where the black curves are the proper profile 
likelihoods and the gray curves are the invalid/attempted profile likelihood 
curves.

```{r plot_prop_var_conf}
# using the right approach 
estimate <- exp(tail(opt_out$par, 2))
estimate <- estimate / (1 + sum(estimate))
par(mar = c(5, 5, 1, 1))
do_plot(pl_genetic_prop, expression(h[G]), estimate[1], identity)

# create curve using the wrong approach
dum_pl <- pl_genetic
dum_pl$xs <- sapply(dum_pl$data, function(x) {
  scales <- exp(c(x$x, tail(x$optim$par, 1)))
  scales[1] / (1 + sum(scales))
})
do_plot(dum_pl, expression(h[G]), estimate[1], identity, col = "gray40", 
        add = TRUE)
# do the same for the environment effect 
do_plot(pl_env_prop, expression(h[E]), estimate[2], identity)

dum_pl <- pl_env
dum_pl$xs <- sapply(dum_pl$data, function(x) {
  scales <- exp(c(x$x, tail(x$optim$par, 1)))
  scales[1] / (1 + sum(scales))
})
do_plot(dum_pl, expression(h[E]), estimate[2], identity, col = "gray40", 
        add = TRUE)
```

It is also possible to pass starting bounds to `pedmod_profile_prop` as shown 
below.

```{r boud_prop_var_conf, cache = 1, message=TRUE}
# confidence interval for the proportion of variance for the genetic effect
pl_genetic_prop_bounds <- pedmod_profile_prop(
  ptr = ll_terms, par = opt_out$par, maxvls = 20000L, 
  minvls = 1000L, alpha = .05, abs_eps = 0, rel_eps = 1e-4, which_prof = 1L,
  use_aprx = TRUE, n_threads = 4L, verbose = TRUE, cluster_weights = c_weights, 
  vls_scales = sqrt(c_weights), bound = c(.3, .65))

# compare the result
pl_genetic_prop_bounds$confs
pl_genetic_prop$confs
```

### Simulation Study

We make a small simulation study below where we are interested in the estimation 
time, bias and coverage of Wald type confidence intervals.

```{r clean_pre_sim_simple_w_ev, echo = FALSE}
rm(list = setdiff(ls(), c("sim_dat", "ped", "fam", "C_env", "std_prop_estimates")))
```

```{r sim_study_simple_w}
# the seeds we will use
seeds <- c(36451989L, 18774630L, 76585289L, 31898455L, 55733878L, 99681114L, 37725150L, 99188448L, 66989159L, 20673587L, 47985954L, 42571905L, 53089211L, 18457743L, 96049437L, 70222325L, 86393368L, 45380572L, 81116968L, 48291155L, 89755299L, 69891073L, 1846862L, 15263013L, 37537710L, 
           25194071L, 14471551L, 38278606L, 55596031L, 5436537L, 75008107L, 83382936L, 50689482L, 71708788L, 52258337L, 23423931L, 61069524L, 24452554L, 32406673L, 14900280L, 24818537L, 59733700L, 82407492L, 95500692L, 62528680L, 88728797L, 9891891L, 36354594L, 69630736L, 41755287L)

# run the simulation study
sim_study <- lapply(seeds, function(s){
  set.seed(s)
  
  # only run the result if it has not been computed
  f <- file.path("cache", "sim_study_simple_w_env", 
                 paste0("simple-w-env-", s, ".RDS"))
  if(!file.exists(f)){
    # simulate the data
    dat <- sim_dat(n_fams = 1000L)
    
    # get the weighted data set
    dat_unqiue <- dat[!duplicated(dat)]
    attributes(dat_unqiue) <- attributes(dat)
    c_weights <- sapply(dat_unqiue, function(x)
      sum(sapply(dat, identical, y = x)))
    rm(dat)
    
    # get the starting values
    library(pedmod)
    do_fit <- function(standardized){
      ll_terms <- pedigree_ll_terms(dat_unqiue, max_threads = 4L)
      ti_start <- system.time(start <- pedmod_start(
        ptr = ll_terms, data = dat_unqiue, n_threads = 4L, 
        cluster_weights = c_weights, standardized = standardized,
        vls_scales = sqrt(c_weights)))
      start$time <- ti_start
      
      # fit the model
      ti_quick <- system.time(
        opt_out_quick <- pedmod_opt(
          ptr = ll_terms, par = start$par, maxvls = 5000L, abs_eps = 0, 
          rel_eps = 1e-2, minvls = 500L, use_aprx = TRUE, n_threads = 4L, 
          cluster_weights = c_weights, standardized = standardized,
          vls_scales = sqrt(c_weights)))
      opt_out_quick$time <- ti_quick
      
      ti_slow <- system.time(
        opt_out <- pedmod_opt(
          ptr = ll_terms, par = opt_out_quick$par, abs_eps = 0, use_aprx = TRUE, 
          n_threads = 4L, cluster_weights = c_weights,
           standardized = standardized, vls_scales = sqrt(c_weights),
          # we changed these parameters
          maxvls = 25000L, rel_eps = 1e-3, minvls = 5000L))
      opt_out$time <- ti_slow
      
      if(standardized){
        start$par     <- standardized_to_direct(start$par        , 2L)
        opt_out$par   <- standardized_to_direct(opt_out$par      , 2L)
        opt_out_quick$par <- standardized_to_direct(opt_out_quick$par, 2L)
      }
      
      if(!standardized){
        hess_time <- system.time(
          hess <- eval_pedigree_hess(
            ptr = ll_terms, par = opt_out$par, maxvls = 25000L, 
            abs_eps = 0, minvls = 5000L, use_aprx = TRUE, 
            rel_eps = 1e-4, n_threads = 4L, cluster_weights = c_weights,
            vls_scales = sqrt(c_weights)))
        attr(hess, "time") <- hess_time
      } else
        hess <- NULL
      
      list(start = start, opt_out = opt_out, opt_out_quick = opt_out_quick, 
           ll_no_rng = start$logLik_no_rng, hess = hess)
    }
    
    fit_direct <- do_fit(standardized = FALSE)
    fit_std    <- do_fit(standardized = TRUE)
    
    saveRDS(list(fit_direct = fit_direct, fit_std = fit_std), f)
  }
  
  # report to console and return 
  out <- readRDS(f)
  message(paste0(capture.output(out$fit_direct$opt_out$par), collapse = "\n"))
  message(paste0(capture.output(out$fit_std   $opt_out$par), collapse = "\n"))
  
  par <- out$fit_direct$opt_out$par
  SEs <- sqrt(diag(attr(out$fit_direct$hess, "vcov")))
  
  message(paste0(capture.output(rbind(
    Estimate = par, SE = SEs)), collapse = "\n"))
  message(sprintf(
    "Time %12.1f, %12.1f. Max ll: %12.4f, %12.4f\n",
    with(out$fit_direct, start$time["elapsed"] + opt_out$time["elapsed"] +
           opt_out_quick$time["elapsed"]),
    with(out$fit_std   , start$time["elapsed"] + opt_out$time["elapsed"]  +
           opt_out_quick$time["elapsed"]),
    -out$fit_direct$opt_out$value,
    -out$fit_std   $opt_out$value))
  
  out
})

# gather the estimates
beta_est <- sapply(sim_study, function(x) 
  cbind(Direct       = head(x$fit_direct$opt_out$par, 2), 
        Standardized = head(x$fit_std   $opt_out$par, 2)), 
  simplify = "array")
sigma_est <- sapply(sim_study, function(x) 
  cbind(Direct       = exp(tail(x$fit_direct$opt_out$par, 2) / 2), 
        Standardized = exp(tail(x$fit_std   $opt_out$par, 2) / 2)), 
  simplify = "array")

# compute the errors
tmp <- sim_dat(2L)
err_beta  <- beta_est  - attr(tmp, "beta")
err_sigma <- sigma_est - sqrt(attr(tmp, "sig_sq"))
dimnames(err_sigma)[[1L]] <- c("std genetic", "std env.")
err <- abind::abind(err_beta, err_sigma, along = 1)

# get the bias estimates and the standard errors
bias <- apply(err, 1:2, mean)
n_sims <- dim(err)[[3]]
SE <- apply(err , 1:2, sd) / sqrt(n_sims)
bias
SE

# make a box plot
b_vals <- expand.grid(rownames(err), strtrim(colnames(err), 1))
box_dat <- data.frame(Error = c(err), 
                      Parameter = rep(b_vals$Var1, n_sims), 
                      Method = rep(b_vals$Var2, dim(err)[[3]]))
par(mar = c(7, 5, 1, 1))
# S is for the standardized and D is for the direct parameterization
boxplot(Error ~ Method + Parameter, box_dat, ylab = "Error", las = 2, 
        xlab = "")
abline(h = 0, lty = 2)
grid()
# get the average computation times
time_vals <- sapply(sim_study, function(x) {
  . <- function(z){
    keep <- c("opt_out", "start")
    out <- setNames(sapply(z[keep], function(z) z$time["elapsed"]), keep)
    c(out, total = sum(out))
  }
  
  rbind(Direct       = .(x$fit_direct), 
        Standardized = .(x$fit_std))
}, simplify = "array")
apply(time_vals, 1:2, mean)
apply(time_vals, 1:2, sd)
apply(time_vals, 1:2, quantile)

# get the standardized errors
ers_sds <- sapply(sim_study, function(x){
  par <- x$fit_direct$opt_out$par
  err <- par - c(attr(tmp, "beta"), log(attr(tmp, "sig_sq")))
  SEs <- sqrt(diag(solve(-x$fit_direct$hess)))
  
  err / SEs
})

rowMeans(abs(ers_sds) < qnorm(.95)) # 90% coverage
rowMeans(abs(ers_sds) < qnorm(.975)) # 95% coverage
rowMeans(abs(ers_sds) < qnorm(.995)) # 99% coverage

# stats for the computation time of the Hessian
hess_time <- sapply(
  sim_study, function(x) attr(x$fit_direct$hess, "time")["elapsed"])
mean(hess_time)
quantile(hess_time, probs = seq(0, 1, .1))

# compute the coverage on the standardized scale with the proportion of 
# variances
ers_sds <- sapply(sim_study, function(x){
  par_n_vcov <- std_prop_estimates(
    x$fit_direct$opt_out$par, 2L, x$fit_direct$hess)
  truth <- std_prop_estimates(c(attr(tmp, "beta"), log(attr(tmp, "sig_sq"))), 2)
  (par_n_vcov$par - truth$par) / sqrt(diag(par_n_vcov$vcov_var))
})

rowMeans(abs(ers_sds) < qnorm(.95)) # 90% coverage
rowMeans(abs(ers_sds) < qnorm(.975)) # 95% coverage
rowMeans(abs(ers_sds) < qnorm(.995)) # 99% coverage
```

## Individual Specific Loadings

The models have used till now are in this form 

$$\begin{align*}
 Y_{ij} &= \begin{cases} 1 & \vec x_{ij}^\top\vec\beta + 
   R_{ij} + \sum_{k = 1}^K \sigma_kU_{ikj} > 0 \\ 
   0 & \text{otherwise} \end{cases} \\
 (U_{ik1}, \dots,  U_{ikn_i})^\top &\sim N^{(n_i)}(\vec 0, C_{ik}) \\
 R_{ij} &\sim N(0, 1)\end{align*}$$
 
for known fixed effects covariates $\vec x_{ij}$ and scale matrices $C_{ij}$.
The $U_{ikj}$ is the $k$'th effect on individual $j$ in cluster $i$. 
For instance, this could be the genetic effect or an environmental effect. 

We may consider the case where all individuals load differently on each of 
the random effects. A model to incorporate such effects is

$$\begin{align*}
 Y_{ij} &= \begin{cases} 1 & \vec x_{ij}^\top\vec\beta + 
   R_{ij} + \sum_{k = 1}^K \sigma_k(\vec z_{ij})U_{ikj} > 0 \\ 
   0 & \text{otherwise} \end{cases} \\
 \sigma_k(\vec z_{ij}) &= \exp(\vec\theta_k^\top\vec z_{ij}) \\
 (U_{ik1}, \dots,  U_{ikn_i})^\top &\sim N^{(n_i)}(\vec 0, C_{ik}) \\
 R_{ij} &\sim N(0, 1)\end{align*}$$
 
where the $\vec z_{ij}$ are known covariates. 
If all the scale matrices are correlation matrices, then this implies that the
proportion of variance attributable to the $l$'th effect for individual $j$ in 
cluster $i$ is 

$$\frac{\sigma_l^2(\vec z_{ij})^2}{1 + \sum_{k = 1}^K\sigma_k^2(\vec z_{ij})^2}$$
rather than

$$\frac{\sigma_l^2}{1 + \sum_{k = 1}^K\sigma_k^2}.$$

The model can equivalent be written as

$$\begin{align*}
 Y_{ij} &= \begin{cases} 1 & \vec x_{ij}^\top\vec\beta + 
   \epsilon_{ij} > 0 \\ 
   0 & \text{otherwise} \end{cases} \\
 \sigma_k(\vec z_{ij}) &= \exp(\vec\theta_k^\top\vec z_{ij}) \\
 D_{ik} &= \text{diag}(\sigma_k(\vec z_{i1}), \dots, \sigma_k(\vec z_{in_i}))\\
 (\epsilon_{i1}, \dots,  \epsilon_{in_i})^\top &\sim 
   N^{(n_i)}\left(\vec 0, I + \sum_{k = 1}^K D_{ik}C_{ik}D_{ik}\right)\end{align*}$$
   
where $\text{diag}(\cdots)$ returns a diagonal matrix. This form is useful for 
simulations. 

As en example, we extend our previous simulation to 

$$\begin{align*}
 Y_{ij} &= \begin{cases} 1 & \beta_0 + \beta_1 B_{ij} + \sigma_E(\vec z_{ij})E_{ij} + \sigma_G(\vec z_{ij})G_{ij} + R_{ij} > 0 \\ 0 & \text{otherwise} \end{cases} \\
 B_{ij} &\sim \text{Bin}(0.1, 1) \\
 (G_{i1}, \dots, G_{in_{i}})^\top &\sim N^{(n_i)}(\vec 0, C_{i1}) \\
(E_{i1}, \dots, E_{in_{i}})^\top &\sim N^{(n_i)}(\vec 0, C_{i2}) \\
 R_{ij} &\sim N(0, 1)\end{align*}$$
 
where $\vec z_{ij}$ is a vector containing an intercept, an indicator for 
whether the individual is a male, and a covariate between minus one and one. 
We will let the heritability for males be larger than for females but the 
environmental effect will be the same given the second covariate. 

```{r clean_pre_loading, echo = FALSE}
rm(list = setdiff(ls(), c("ped", "fam", "C_env")))
```

We assign the new simulation function below:

```{r loading_assign_sim_dat}
# the covariates for the scale parameters, Z
vcov_covs <- cbind(intercept = rep(1, 10), is_male = rep(1:0, 5), 
                   cov = seq(-1, 1, length.out = 10))
vcov_covs

# set the parameters we will use
beta <- c(-2, 4)
thetas <- matrix(c(-0.394228680182135, 1.12739721457885, 1,
                   -0.50580045583924, 0.64964149206513, -1), 3)

# we can compute the individual specific proportion of variances as follows
scales <- exp(vcov_covs %*% thetas)
cbind(scales^2, 1) / rowSums(cbind(scales^2, 1))

# the heritability differs between males and females but the environmental 
# effect is the same given the second covariate as shown below
vcov_covs_tmp <- vcov_covs
vcov_covs_tmp[, 3] <- 0
scales <- exp(vcov_covs_tmp %*% thetas)
cbind(scales^2, 1) / rowSums(cbind(scales^2, 1))

# simulates a data set. 
# 
# Args:
#   n_fams: number of families.
#   beta: the fixed effect coefficients.
#   thetas: the coefficients for the scale parameters.
sim_dat <- function(n_fams, beta, thetas){
  # setup before the simulations
  Cmat <- 2 * kinship(ped)
  n_obs <- NROW(fam)
  
  scales <- exp(vcov_covs %*% thetas)
  Sig <- diag(n_obs) + diag(scales[, 1]) %*% Cmat %*% diag(scales[, 1]) + 
    diag(scales[, 2]) %*% C_env %*% diag(scales[, 2])
  Sig_chol <- chol(Sig)
  
  # simulate the data
  out <- replicate(
    n_fams, {
      # simulate covariates
      X <- cbind(`(Intercept)` = 1, Binary = runif(n_obs) > .9)
      
      # assign the linear predictor + noise
      eta <- drop(X %*% beta) + drop(rnorm(n_obs) %*% Sig_chol)
      
      # return the list in the format needed for the package. We also have to 
      # pass the covariates for the scale parameters
      list(y = as.numeric(eta > 0), X = X, Z = vcov_covs, scale_mats = list(
        Genetic = Cmat, Environment = C_env))
    }, simplify = FALSE)
  
  # add attributes with the true values and return 
  attributes(out) <- list(beta = beta, thetas = thetas)
  out
}
```

A data set is sampled below and the model is estimated.

```{r loading_one_dat, cache = 1}
# simulate a data set
set.seed(72466753)
dat <- sim_dat(n_fams = 1000L, beta = beta, thetas = thetas)

# evaluate the log marginal likelihood at the true parameters
library(pedmod)
ll_terms_wo_weights <- pedigree_ll_terms_loadings(dat, max_threads = 4L)

logLik_truth <- eval_pedigree_ll(
  ll_terms_wo_weights, c(beta, thetas), maxvls = 25000L, minvls = 1000L, 
  abs_eps = 0, rel_eps = 1e-3, n_threads = 4L)

# remove the duplicated terms and use weights. This can be done more efficiently
# and may not catch all duplicates
dat_unqiue <- dat[!duplicated(dat)]
length(dat_unqiue) # number of unique terms

# get the weights. This can be written in a much more efficient way
c_weights <- sapply(dat_unqiue, function(x)
  sum(sapply(dat, identical, y = x)))

# evaluate log likelihood again and show that we got the same
ll_terms <- pedigree_ll_terms_loadings(dat_unqiue, max_threads = 4L)

logLik_truth_weighted <- eval_pedigree_ll(
  ll_terms, c(beta, thetas), maxvls = 25000L, minvls = 1000L, 
  abs_eps = 0, rel_eps = 1e-3, n_threads = 4L, cluster_weights = c_weights)

print(logLik_truth_weighted, digits = 8)
print(logLik_truth, digits = 8)

# note that the variance is greater for the weighted version
ll_ests <- sapply(1:50, function(seed){
  set.seed(seed)
  eval_pedigree_ll(
    ll_terms_wo_weights, c(beta, thetas), maxvls = 10000L, minvls = 1000L, 
    abs_eps = 0, rel_eps = 1e-3, n_threads = 4L)
})
ll_ests_fast <- sapply(1:50, function(seed){
  set.seed(seed)
  eval_pedigree_ll(
    ll_terms, c(beta, thetas), maxvls = 10000L, minvls = 1000L, 
    abs_eps = 0, rel_eps = 1e-3, n_threads = 4L, cluster_weights = c_weights)
})

# the estimates are comparable
c(`Without weights` = mean(ll_ests), `With weights` = mean(ll_ests_fast))

# the standard deviation is different
c(`Without weights` = sd(ll_ests), `With weights` = sd(ll_ests_fast))

# we can mitigate this by using the vls_scales argument which though is a bit 
# slower
ll_ests_fast_vls_scales <- sapply(1:50, function(seed){
  set.seed(seed)
  eval_pedigree_ll(
    ll_terms, c(beta, thetas), maxvls = 10000L, minvls = 1000L, 
    abs_eps = 0, rel_eps = 1e-3, n_threads = 4L, cluster_weights = c_weights, 
    vls_scales = sqrt(c_weights))
})

# the estimates are comparable
c(`Without weights` = mean(ll_ests), `With weights` = mean(ll_ests_fast), 
  `With weights and vls_scales` = mean(ll_ests_fast_vls_scales))

# the standard deviation is different
c(`Without weights` = sd(ll_ests), `With weights` = sd(ll_ests_fast), 
  `With weights and vls_scales` = sd(ll_ests_fast_vls_scales))

# get the starting values
system.time(start <- pedmod_start_loadings(
  ll_terms, data = dat_unqiue, cluster_weights = c_weights))

# find the maximum likelihood estimator
system.time(
  opt_res <- pedmod_opt(
    ll_terms, par = start$par, maxvls = 25000L, minvls = 5000L, 
    abs_eps = 0, rel_eps = 1e-3, n_threads = 4L, use_aprx = TRUE, 
    cluster_weights = c_weights, vls_scales = sqrt(c_weights)))
```

We compare the maximum likelihood estimator with the true values below.

```{r loading_show_res}
# the fixed effects
rbind(Truth = beta, 
      Start = head(start$par, 2), 
      Estimate = head(opt_res$par, 2))

# the scale coefficients
array(c(thetas, tail(start$par, -2), tail(opt_res$par, -2)), 
      dim = c(dim(thetas), 3L), 
      dimnames = list(NULL, NULL, c("Truth", "Start", "Estimate")))

# compare the proportion of variance for the individual. First the estimates
thetas_est <- matrix(tail(opt_res$par, -2), NCOL(vcov_covs))
scales <- exp(vcov_covs %*% thetas_est)
cbind(scales^2, 1) / rowSums(cbind(scales^2, 1))

# then the true proportions
scales <- exp(vcov_covs %*% thetas)
cbind(scales^2, 1) / rowSums(cbind(scales^2, 1))

# the log likelihood at the true parameters and at the estimate
print(logLik_truth_weighted, digits = 8)
print(-opt_res$value, digits = 8)
```

### Profile Likelihood

We can construct a profile likelihood for the parameters like before. 
For instance, we can look at the scale parameter for the heritability shift 
for the males with the following code. 

```{r pre_loadings_pl, echo = FALSE}
ll_terms <- pedigree_ll_terms_loadings(dat_unqiue, max_threads = 4L)
```

```{r loadings_pl, cache=1, message=TRUE}
system.time(
  pl_curve <- pedmod_profile(
    ll_terms, par = opt_res$par, maxvls = 25000L, minvls = 5000L, 
    abs_eps = 0, rel_eps = 1e-3, n_threads = 4L, use_aprx = TRUE, 
    cluster_weights = c_weights, vls_scales = sqrt(c_weights), 
    delta = .2, verbose = TRUE, which_prof = 4L))
```

The confidence interval is shown below along with a plot of the profile 
likelihood curve.

```{r show_loadings_pl_show}
pl_curve$confs # the confidence interval

# plot the profile likelihood curve
local({
  max_diff <- 4
  xs <- pl_curve$xs
  pls <- pl_curve$p_log_Lik
  keep <- pls > max(pls) - max_diff
  xs <- xs[keep]
  pls <- pls[keep]

  par(mar = c(5, 5, 1, 1))  
  plot(xs, pls, bty = "l", pch = 16, xlab = expression(theta[2]), 
       ylab = "Profile likelihood")
  grid()
  abline(v = opt_res$par[4], lty = 2) # the estimate
  # mark the critical value
  abline(h = max(pls) - qchisq(.95, 1) / 2, lty = 3) 
  
  lines(spline(xs, pls, n = 100L))
})
```

Some of the quantities of interest are nonlinear functions of the parameters, 
however. 
For instance, we may be interested in the difference in the proportion of 
variance for males at `cov = 0`. We can construct a profile likelihood based 
confidence interval for this difference but this requires an optimizer that 
supports nonlinear equality constraints. The `pedmod_profile_nleq` function is 
created for this purpose and an example of using it to compute the 
aforementioned difference is shown below.

```{r loadings_nleq_pl, cache = 1, message=TRUE}
# computes the difference between the male and females heritability at 
# cov = 0
heq <- function(par){
  theta <- matrix(tail(par, 6), 3)
  scs <- matrix(c(1, 1, 0, 1, 0, 0), 2) %*% theta
  scs <- exp(scs)
  prop_genetic <- scs[, 1]^2 / (1 + rowSums(scs^2))
  diff(prop_genetic)
}
heq(opt_res$par)

# construct the profile likelihood curve
system.time(
  pl_curve_nleq <- pedmod_profile_nleq(
    ll_terms, par = opt_res$par, maxvls = 5000L, minvls = 1000L, 
    abs_eps = 0, rel_eps = 1e-3, n_threads = 4L, use_aprx = TRUE, 
    cluster_weights = c_weights, vls_scales = sqrt(c_weights), 
    delta = .2, verbose = TRUE, heq = heq, heq_bounds = c(-1, 1)))
```

The confidence interval is shown below along with a plot of the profile 
likelihood curve.

```{r show_loadings_nleq_pl}
pl_curve_nleq$confs # the confidence interval

# plot the profile likelihood curve
local({
  max_diff <- 4
  xs <- pl_curve_nleq$xs
  pls <- pl_curve_nleq$p_log_Lik
  keep <- pls > max(pls) - max_diff
  xs <- xs[keep]
  pls <- pls[keep]

  par(mar = c(5, 5, 1, 1))  
  plot(xs, pls, bty = "l", pch = 16, ylab = "Profile likelihood",
       xlab = "Heritability difference at cov = 0")
  grid()
  abline(v = opt_res$par[4], lty = 2) # the estimate
  # mark the critical value
  abline(h = max(pls) - qchisq(.95, 1) / 2, lty = 3) 
  
  lines(spline(xs, pls, n = 100L))
})
```

### Simulation Study
We make a small simulation study below where we are interested in the estimation time and bias.

```{r loadings_sim_study}
# the seeds we will use
seeds <- c(36451989L, 18774630L, 76585289L, 31898455L, 55733878L, 99681114L, 37725150L, 99188448L, 66989159L, 20673587L, 47985954L, 42571905L, 53089211L, 18457743L, 96049437L, 70222325L, 86393368L, 45380572L, 81116968L, 48291155L, 89755299L, 69891073L, 1846862L, 15263013L, 37537710L, 
           25194071L, 14471551L, 38278606L, 55596031L, 5436537L, 75008107L, 83382936L, 50689482L, 71708788L, 52258337L, 23423931L, 61069524L, 24452554L, 32406673L, 14900280L, 24818537L, 59733700L, 82407492L, 95500692L, 62528680L, 88728797L, 9891891L, 36354594L, 69630736L, 41755287L)

# run the simulation study
sim_study <- lapply(seeds, function(s){
  set.seed(s)
  
  # only run the result if it has not been computed
  f <- file.path("cache", "sim_study_loadings", 
                 paste0("loadings-", s, ".RDS"))
  if(!file.exists(f)){
    # simulate the data
    dat <- sim_dat(n_fams = 1000L, beta = beta, thetas = thetas)
    
    # get the weighted data set
    dat_unqiue <- dat[!duplicated(dat)]
    attributes(dat_unqiue) <- attributes(dat)
    c_weights <- sapply(dat_unqiue, function(x)
      sum(sapply(dat, identical, y = x)))
    rm(dat)
    
    # get the starting values
    library(pedmod)
    ll_terms <- pedigree_ll_terms_loadings(dat_unqiue, max_threads = 4L)
    
    # fit the model
    ti_start <- system.time(start <- pedmod_start_loadings(
        ptr = ll_terms, data = dat_unqiue, cluster_weights = c_weights))
    start$time <- ti_start
    
    ti_quick <- system.time(
      opt_out_quick <- pedmod_opt(
        ptr = ll_terms, par = start$par, maxvls = 5000L, abs_eps = 0, 
        rel_eps = 1e-2, minvls = 500L, use_aprx = TRUE, n_threads = 4L, 
        cluster_weights = c_weights, vls_scales = sqrt(c_weights)))
    opt_out_quick$time <- ti_quick
    
    ti_slow <- system.time(
      opt_out <- pedmod_opt(
        ptr = ll_terms, par = opt_out_quick$par, abs_eps = 0, use_aprx = TRUE, 
        n_threads = 4L, cluster_weights = c_weights, 
        vls_scales = sqrt(c_weights),
        # we changed these parameters
        maxvls = 25000L, rel_eps = 1e-3, minvls = 5000L))
    opt_out$time <- ti_slow
    
    saveRDS(list(start = start, opt_out_quick = opt_out_quick, 
                 opt_out = opt_out), f)
    
  }
  
  # report to console and return 
  out <- readRDS(f)
  message(paste0(capture.output(
    rbind(Estimate = out$opt_out$par, Truth = c(beta, thetas))), 
    collapse = "\n"))

  message(sprintf(
    "Time %12.1f. Max ll: %12.4f\n",
    with(out, start$time["elapsed"] + opt_out$time["elapsed"] +
           opt_out_quick$time["elapsed"]),
    -out$opt_out$value))
  
  out
})

# compute the bias estimates
estimates <- sapply(sim_study, function(x) x$opt_out$par)
rownames(estimates) <- c("(Intercept)", "Binary",
                         paste0("Genetic", 1:3), 
                         paste0("Env", 1:3))

err <- estimates - c(beta, thetas)
rbind(Bias = rowMeans(err), SE = apply(err, 1, sd) / sqrt(NCOL(err)))

# make a box plot
par(mar = c(7, 5, 1, 1))
# S is for the standardized and D is for the direct parameterization
boxplot(t(err), ylab = "Error", las = 2)
abline(h = 0, lty = 2)
grid()
# summary stats for the computation time
comp_times <- sapply(
  sim_study, function(x) sapply(x, `[[`, "time")["elapsed", ])
summary(t(comp_times))
summary(colSums(comp_times))
```

## More Complicated Example

```{r clean_up_complicated, echo = FALSE}
rm(list = ls())
```

We consider a more complicated example in this section and use some of the lower
level functions in the package as an example.
We start by sourcing a file to get a function to simulate a data set with 
a maternal effect and a genetic effect like in @Mahjani20:

```{r source_sim_file}
# source the file to get the simulation function
source(system.file("gen-pedigree-data.R", package = "pedmod"))

# simulate a data set
set.seed(68167102)
dat <- sim_pedigree_data(n_families = 1000L)

# distribution of family sizes
par(mar = c(5, 4, 1, 1))
plot(table(sapply(dat$sim_data, function(x) length(x$y))), 
     xlab = "Family size", ylab = "Number of families", bty = "l")
# total number of observations
sum(sapply(dat$sim_data, function(x) length(x$y)))

# average event rate
mean(unlist(sapply(dat$sim_data, `[[`, "y")))
```

As @Mahjani20, we have data families linked by three generations but we 
only have data for the last generation. We illustrate this with the first 
family in the simulated data set:

```{r one_family}
# this is the full family 
library(kinship2)
fam1 <- dat$sim_data[[1L]]
plot(fam1$pedAll)
# here is the C matrix for the genetic effect
rev_img <- function(x, ...)
  image(x[, NROW(x):1], ...)
cl <- colorRampPalette(c("Red", "White", "Blue"))(101)

par(mar = c(2, 2, 1, 1))
rev_img(fam1$rel_mat, xaxt = "n", yaxt = "n", col = cl, 
        zlim = c(-1, 1))
# the first part of the matrix is given below
with(fam1, 
     Matrix::Matrix(rel_mat[seq_len(min(10, NROW(rel_mat))), 
                            seq_len(min(10, NROW(rel_mat)))],
                    sparse = TRUE))

# here is the C matrix for the maternal effect
rev_img(fam1$met_mat, xaxt = "n", yaxt = "n", col = cl, 
        zlim = c(-1, 1))
# the first part of the matrix is given below
with(fam1, 
     Matrix::Matrix(met_mat[seq_len(min(10, NROW(met_mat))), 
                            seq_len(min(10, NROW(met_mat)))],
                    sparse = TRUE))

# each simulated family has such two matrices in addition to a design matrix
# for the fixed effects, X, and a vector with outcomes, y
str(fam1[c("X", "y")])
```

Then we perform the model estimation:

<!-- knitr::opts_knit$set(output.dir = ".") -->
<!-- knitr::load_cache("est_mod", path = "cache/README-") -->

```{r est_mod, cache=1}
# the true parameters are
dat$beta
dat$sc # the sigmas squared

# prepare the data to pass to the functions in the package
dat_arg <- lapply(dat$sim_data, function(x){
  # we need the following for each family: 
  #   y: the zero-one outcomes.
  #   X: the design matrix for the fixed effects. 
  #   scale_mats: list with the scale matrices for each type of effect.
  list(y = as.numeric(x$y), X = x$X,
       scale_mats = list(x$rel_mat, x$met_mat))
})

# create a C++ object
library(pedmod)
ll_terms <- pedigree_ll_terms(dat_arg, max_threads = 4L)

# get the starting values. This is very fast
y <- unlist(lapply(dat_arg, `[[`, "y"))
X <- do.call(rbind, lapply(dat_arg, `[[`, "X"))
start_fit <-  glm.fit(X, y, family = binomial("probit"))

# log likelihood at the starting values without random effects
-sum(start_fit$deviance) / 2     
(beta <- start_fit$coefficients) # starting values for fixed effects 

# start at moderate sized scale parameters
sc <- rep(log(.2), 2)

# check log likelihood at the starting values. First we assign a function 
# to approximate the log likelihood and the gradient
fn <- function(par, seed = 1L, rel_eps = 1e-2, use_aprx = TRUE, 
               n_threads = 4L, indices = NULL, maxvls = 25000L, 
               method = 0L, use_sparse = FALSE, use_tilting = FALSE){
  set.seed(seed)
  -eval_pedigree_ll(
    ptr = if(use_sparse) ll_terms_sparse else ll_terms, par = par, 
    maxvls = maxvls, abs_eps = 0, rel_eps = rel_eps, minvls = 1000L, 
    use_aprx = use_aprx, n_threads = n_threads, indices = indices, 
    method = method, use_tilting = use_tilting)
}
gr <- function(par, seed = 1L, rel_eps = 1e-2, use_aprx = TRUE, 
               n_threads = 4L, indices = NULL, maxvls = 25000L, 
               method = 0L, use_sparse = FALSE, use_tilting = FALSE){
  set.seed(seed)
  out <- -eval_pedigree_grad(
    ptr = if(use_sparse) ll_terms_sparse else ll_terms, par = par, 
    maxvls = maxvls, abs_eps = 0, rel_eps = rel_eps, minvls = 1000L, 
    use_aprx = use_aprx, n_threads = n_threads, indices = indices, 
    method = method, use_tilting = use_tilting)
  structure(c(out), value = -attr(out, "logLik"), 
            n_fails = attr(out, "n_fails"), 
            std = attr(out, "std"))
}

# check output at the starting values
system.time(ll <- -fn(c(beta, sc)))
ll # the log likelihood at the starting values
system.time(gr_val <- gr(c(beta, sc)))
gr_val # the gradient at the starting values

# standard deviation of the approximation
sd(sapply(1:25, function(seed) fn(c(beta, sc), seed = seed)))

# we do the same for the gradient elements but only for a subset of the 
# log marginal likelihood elements
gr_hats <- sapply(
  1:25, function(seed) gr(c(beta, sc), seed = seed, indices = 0:99))
apply(gr_hats, 1, sd)

# the errors are on similar magnitudes
gr(c(beta, sc), indices = 0:99)

# verify the gradient (may not be exactly equal due to MC error)
rbind(numDeriv = numDeriv::grad(fn, c(beta, sc), indices = 0:10), 
      pedmod   = gr(c(beta, sc), indices = 0:10))

# optimize the log likelihood approximation
system.time(opt <- optim(c(beta, sc), fn, gr, method = "BFGS"))
```

The output from the optimization is shown below:

```{r show_est_mod}
print(-opt$value, digits = 8) # the maximum log likelihood
opt$convergence               # check convergence

# compare the estimated fixed effects with the true values
rbind(truth     = dat$beta, 
      estimated = head(opt$par, length(dat$beta)))

# compare estimated scale parameters with the true values
rbind(truth     = dat$sc, 
      estimated = exp(tail(opt$par, length(dat$sc))))
```

```{r svrg, eval = FALSE, echo = FALSE}
#####
# performs stochastic gradient descent instead (using SVRG).
#
# Args:
#   par: starting value.
#   gr: gradient function which return the function value as an attribute
#       called value.
#   n_clust: number of clusters.
#   batch_size: number of observations in each batch.
#   maxit: maximum number of iteration.
#   lr: learning rate.
#   verbose: print output during the estimation.
#   decay: numeric scalar used to decrease the learning rate.
#   ...: arguments passed to gr.
svrg <- function(par, gr, n_clust, batch_size, maxit = 10L, 
                 lr, verbose = FALSE, decay = .98, ...){
  indices <- sample.int(n_clust, replace = FALSE) - 1L
  blocks <- tapply(indices, (seq_along(indices) - 1L) %/% batch_size,
                   identity, simplify = FALSE)

  n_blocks <- length(blocks)
  n_par <- length(par)
  estimates <- matrix(NA_real_, n_par, maxit + 1L)
  fun_vals <- numeric(maxit + 1L)
  estimates[, 1L] <- par

  lr_use <- lr / decay
  V_mult <- qnorm(1 - .99 / maxit)
  for(k in 1:maxit + 1L){
    old_val <- estimates[, k - 1L]
    old_grs <- sapply(1:n_blocks - 1L, function(ii){
      idx_b <- (ii %% n_blocks) + 1L
      res_old <- gr(old_val, indices = blocks[[idx_b]], ...)
      c(attr(res_old, "value"), res_old)
    })

    fun_vals[k - 1L] <- sum(old_grs[1, ])
    old_grs <- old_grs[-1L, , drop = FALSE ]
    old_gr <- rowSums(old_grs) / n_blocks

    lr_use <- lr_use * decay
    for(ii in 1:n_blocks - 1L){
      idx_b <- (ii %% n_blocks) + 1L
      res <- gr(par, indices = blocks[[idx_b]], ...)
      fun_vals[k] <- fun_vals[k] + attr(res, "value")
      dir <- res - old_grs[, ii + 1L] + old_gr

      par <- par - lr_use * dir
    }

    estimates[, k] <- par

    if(verbose)
      cat(
        sprintf("End if iteration %4d with learning rate %.8f", k - 1L,
                lr_use),
        sprintf("Log marginal likelihood approximation is %12.2f", fun_vals[k]),
        sprintf("Previous approximate gradient norm was %14.2f\n",
                n_blocks * norm(as.matrix(old_gr))),
        sep = "\n")

    old_v <- fun_vals[k - 1L]
    new_v <- fun_vals[k]
  }

  list(result = par, fun_vals = fun_vals[2:k],
       estimates = estimates[, 2:k, drop = FALSE])
}

set.seed(1)
svrg_res <- svrg(c(beta, sc), gr = gr, n_clust = length(dat_arg), 
                 batch_size = 200L, lr = 2e-3, verbose = TRUE, maxit = 50L)
```

### Computation in Parallel

```{r pre_comp_par, echo = FALSE}
library(pedmod)
ll_terms <- pedigree_ll_terms(dat_arg, max_threads = 4L)
```

The method scales reasonably well in the number of threads as 
shown below:

```{r time_mult, cache = 1}
library(microbenchmark)
microbenchmark(
  `fn (1 thread)`  = fn(c(beta, sc), n_threads = 1),
  `fn (2 threads)` = fn(c(beta, sc), n_threads = 2),
  `fn (4 threads)` = fn(c(beta, sc), n_threads = 4),
  `gr (1 thread)`  = gr(c(beta, sc), n_threads = 1),
  `gr (2 threads)` = gr(c(beta, sc), n_threads = 2),
  `gr (4 threads)` = gr(c(beta, sc), n_threads = 4),
  times = 1)
```

### Using ADAM
We use stochastic gradient descent with the ADAM method [@Kingma15] in 
this section. We define a function below to apply ADAM and use it 
to estimate the model.

<!-- knitr::opts_knit$set(output.dir = ".") -->
<!-- knitr::load_cache("use_adam", path = "cache/README-") -->

```{r re_create_ptr, echo = FALSE}
library(pedmod)
ll_terms <- pedigree_ll_terms(dat_arg, max_threads = 4L)
```

```{r use_adam, cache = 1}
#####
# performs stochastic gradient descent (using ADAM).
#
# Args:
#   par: starting value.
#   gr: function to evaluate the log marginal likelihood.
#   n_clust: number of observation.
#   n_blocks: number of blocks.
#   maxit: maximum number of iteration.
#   seed: seed to use.
#   epsilon, alpha, beta_1, beta_2: ADAM parameters.
#   maxvls: maximum number of samples to draw in each iteration. Thus, it 
#           needs maxit elements.
#   verbose: print output during the estimation.
#   ...: arguments passed to gr.
adam <- function(par, gr, n_clust, n_blocks, maxit = 10L,
                 seed = 1L, epsilon = 1e-8, alpha = .001, beta_1 = .9,
                 beta_2 = .999, maxvls = rep(10000L, maxit), 
                 verbose = FALSE, ...){
  grp_dummy <- (seq_len(n_clust) - 1L) %% n_blocks
  n_par <- length(par)
  m <- v <- numeric(n_par)
  fun_vals <- numeric(maxit)
  estimates <- matrix(NA_real_, n_par, maxit)
  i <- -1L

  for(k in 1:maxit){
    # sample groups
    indices <- sample.int(n_clust, replace = FALSE) - 1L
    blocks <- tapply(indices, grp_dummy, identity, simplify = FALSE)
    
    for(ii in 1:n_blocks){
      i <- i + 1L
      idx_b <- (i %% n_blocks) + 1L
      m_old <- m
      v_old <- v
      res <- gr(par, indices = blocks[[idx_b]], maxvls = maxvls[k])
      fun_vals[(i %/% n_blocks) + 1L] <-
        fun_vals[(i %/% n_blocks) + 1L] + attr(res, "value")
      res <- c(res)

      m <- beta_1 * m_old + (1 - beta_1) * res
      v <- beta_2 * v_old + (1 - beta_2) * res^2

      m_hat <- m / (1 - beta_1^(i + 1))
      v_hat <- v / (1 - beta_2^(i + 1))

      par <- par - alpha * m_hat / (sqrt(v_hat) + epsilon)
    }
    
    if(verbose){
      cat(sprintf("Ended iteration %4d. Running estimate of the function value is: %14.2f\n", 
                  k, fun_vals[k]))
      cat("Parameter estimates are:\n")
      cat(capture.output(print(par)), sep = "\n")
      cat("\n")
    }

    estimates[, k] <- par
  }

  list(par = par, estimates = estimates, fun_vals = fun_vals)
}

#####
# use the function
# assign the maximum number of samples we will use
maxit <- 100L
minvls <- 250L
maxpts <- formals(gr)$maxvls
maxpts_use <- exp(seq(log(2 * minvls), log(maxpts), length.out = maxit))

# show the maximum number of samples we use
par(mar = c(5, 4, 1, 1))
plot(maxpts_use, pch = 16, xlab = "Iteration number", bty = "l",
     ylab = "Maximum number of samples", ylim = range(0, maxpts_use))
set.seed(1)
system.time(
  adam_res <- adam(c(beta, sc), gr = gr, n_clust = length(dat_arg), 
                   n_blocks = 10L, alpha = 1e-2, maxit = maxit, 
                   verbose = FALSE, maxvls = maxpts_use, 
                   minvls = minvls))
```

The result is shown below.

```{r adam_re_create_ptr, echo = FALSE}
library(pedmod)
ll_terms <- pedigree_ll_terms(dat_arg, max_threads = 4L)
fn <- function(par, seed = 1L, rel_eps = 1e-2, use_aprx = TRUE, 
               n_threads = 4L, indices = NULL, maxvls = 25000L){
  set.seed(seed)
  -eval_pedigree_ll(
    ptr = ll_terms, par = par, maxvls = maxvls, abs_eps = 0, rel_eps = rel_eps, 
    minvls = 1000L, use_aprx = use_aprx, n_threads = n_threads, 
    indices = indices)
}
```

```{r res_adam}
print(-fn(adam_res$par), digits = 8) # the maximum log likelihood

# compare the estimated fixed effects with the true values
rbind(truth             = dat$beta,
      `estimated optim` = head(opt$par     , length(dat$beta)),
      `estimated ADAM`  = head(adam_res$par, length(dat$beta)))

# compare estimated scale parameters with the true values
rbind(truth             = dat$sc, 
      `estimated optim` = exp(tail(opt$par     , length(dat$sc))), 
      `estimated ADAM`  = exp(tail(adam_res$par, length(dat$sc))))

# could possibly have stopped much earlier maybe. Dashed lines are final 
# estimates
par(mar = c(5, 4, 1, 1))
matplot(t(adam_res$estimates), type = "l", col = "Black", lty = 1, 
        bty = "l", xlab = "Iteration", ylab = "Estimate")
for(s in adam_res$par)
  abline(h = s, lty = 2)
```

### The Multivariate Normal CDF Approximation

We compare the multivariate normal CDF approximation in this section 
with the approximation from the mvtnorm package which uses the implementation 
by @Genz02. The same algorithm is used 
but the version in this package is re-written in C++ and differs slightly.
Moreover, we have implemented an approximation of the standard normal CDF
and its inverse which reduces the computation time as we will show below.

We also compare our implementation of the minimax titling method suggested 
by @Botev17 with the implementation in the TruncatedNormal package.

```{r compare_w_mvtnorm, cache = 1}
#####
# settings for the simulation study
library(mvtnorm)
library(pedmod)
library(microbenchmark)
set.seed(78459126)
n <- 5L         # number of variables to integrate out
rel_eps <- 1e-4 # the relative error to use

#####
# run the simulation study
sim_res <- replicate(expr = {
  # simulate covariance matrix and the upper bound
  S <- drop(rWishart(1L, 2 * n, diag(n) / 2 / n))
  u <- rnorm(n)
  
  # function to use pmvnorm
  use_mvtnorm <- function(rel_eps)
    mvtnorm::pmvnorm(
      upper = u, sigma = S, algorithm = GenzBretz(
      abseps = 0, releps = rel_eps, maxpts = 1e7))
  
  # function to use pmvnorm from TruncatedNormal
  use_trunc_norm <- function(n_sample)
    TruncatedNormal::pmvnorm(
      sigma = S, lb = rep(-Inf, n), ub = u, type = "qmc", B = n_sample)
  
  # function to use this package
  use_mvndst <- function(use_aprx = FALSE, method = 0L, use_tilting = TRUE)
    mvndst(lower = rep(-Inf, n), upper = u, mu = rep(0, n), 
           sigma = S, use_aprx = use_aprx, abs_eps = 0, rel_eps = rel_eps,
           maxvls = 1e7, method = method, use_tilting = use_tilting)

  # get a very precise estimate
  truth <- use_mvtnorm(rel_eps / 100)
  
  # computes the error with repeated approximations and compute the time it
  # takes
  n_rep <- 5L
  run_n_time <- function(expr){
    expr <- substitute(expr)
    ti <- get_nanotime()
    res <- replicate(n_rep, eval(expr))
    ti <- get_nanotime() - ti
    err <- (res - truth) / truth
    c(SE = sqrt(sum(err^2) / n_rep), time = ti / n_rep / 1e9)
  }
  
  mvtnorm_res                <- run_n_time(use_mvtnorm(rel_eps))
  
  n_sample <- attr(use_mvndst(TRUE, method = 0L, use_tilting = TRUE), "n_it")
  TruncatedNormal_res        <- run_n_time(use_trunc_norm(n_sample))
  
  mvndst_no_aprx_res_Korobov <- 
    run_n_time(use_mvndst(FALSE, method = 0L, use_tilting = FALSE))
  mvndst_w_aprx_res_Korobov  <- 
    run_n_time(use_mvndst(TRUE , method = 0L, use_tilting = FALSE))
  mvndst_no_aprx_res_Sobol   <- 
    run_n_time(use_mvndst(FALSE, method = 1L, use_tilting = FALSE))
  mvndst_w_aprx_res_Sobol    <- 
    run_n_time(use_mvndst(TRUE , method = 1L, use_tilting = FALSE))
  
  mvndst_no_aprx_res_Korobov_tilt <- 
    run_n_time(use_mvndst(FALSE, method = 0L, use_tilting = TRUE))
  mvndst_w_aprx_res_Korobov_tilt  <- 
    run_n_time(use_mvndst(TRUE , method = 0L, use_tilting = TRUE))
  mvndst_no_aprx_res_Sobol_tilt   <- 
    run_n_time(use_mvndst(FALSE, method = 1L, use_tilting = TRUE))
  mvndst_w_aprx_res_Sobol_tilt    <- 
    run_n_time(use_mvndst(TRUE , method = 1L, use_tilting = TRUE))

  # return 
  rbind(mvtnorm            = mvtnorm_res, 
        TruncatedNormal = TruncatedNormal_res,
        `no aprx; Korobov` = mvndst_no_aprx_res_Korobov, 
        `no aprx; Sobol` = mvndst_no_aprx_res_Sobol, 
        `w/ aprx; Korobov` = mvndst_w_aprx_res_Korobov,
        `w/ aprx; Sobol` = mvndst_w_aprx_res_Sobol,
        
        `no aprx; Korobov (tilt)` = mvndst_no_aprx_res_Korobov_tilt, 
        `no aprx; Sobol (tilt)` = mvndst_no_aprx_res_Sobol_tilt, 
        `w/ aprx; Korobov (tilt)` = mvndst_w_aprx_res_Korobov_tilt,
        `w/ aprx; Sobol (tilt)` = mvndst_w_aprx_res_Sobol_tilt)
}, n = 100, simplify = "array")
```

Box plots of the relative errors are shown below:

```{r show_averge_rel_err, fig.height = knitr::opts_chunk$get("fig.height") * 4/3}
rowMeans(sim_res[, "SE", ])
par(mar = c(10, 4, 1, 1), bty = "l")
boxplot(t(sim_res[, "SE", ]), las = 2)
grid()
```

The new implementation is faster when the approximation is used:

```{r use_new_impl, fig.height = knitr::opts_chunk$get("fig.height") * 4/3}
rowMeans(sim_res[, "time", ])
par(mar = c(9, 4, 1, 1), bty = "l")
boxplot(t(sim_res[, "time", ]), log = "y", las = 2)
grid()
```

Next, we compare the methods with the first example from @Botev17. This is with 
a low probability case and we would expect the minimax tilted version to 
perform better. We fix the number of samples with all packages in this example.

```{r botev_example, cache = 1}
# settings for the test like in Botev (2017)
library(mvtnorm)
library(pedmod)
library(microbenchmark)
ds <- c(3, 5, 10, 15, 20, 25)
n_sample <- 10000L

# run the simulation study
set.seed(15418038)
sim_res <- sapply(ds, \(d){
  S <- solve(diag(1/2, d) + 1/2)
  l <- rep(1/2, d)
  u <- rep(1, d)
  
  # function to use pmvnorm
  use_mvtnorm <- function(n_sample)
    mvtnorm::pmvnorm(lower = l, upper = u, sigma = S, algorithm = GenzBretz(
            abseps = 0, releps = 0, maxpts = n_sample))
  
  # function to use pmvnorm from TruncatedNormal
  use_trunc_norm <- function(n_sample)
    TruncatedNormal::pmvnorm(
      sigma = S, lb = l, ub = u, type = "qmc", B = n_sample)
  
  # function to use this package
  use_mvndst <- function(use_aprx = FALSE, method = 0L, use_tilting = TRUE)
    mvndst(lower = l, upper = u, mu = rep(0, d), 
           sigma = S, use_aprx = use_aprx, abs_eps = 0, rel_eps = 0,
           maxvls = n_sample, method = method, use_tilting = use_tilting, 
           minvls = n_sample)

  # get a very precise estimate
  truth <- use_trunc_norm(n_sample * 100L)
  
  # computes the error with repeated approximations and compute the time it
  # takes
  n_rep <- 25L
  run_n_time <- function(expr){
    expr <- substitute(expr)
    ti <- get_nanotime()
    res <- replicate(n_rep, eval(expr))
    ti <- get_nanotime() - ti
    err <- (res - truth) / truth
    c(SE = sqrt(sum(err^2) / n_rep), time = ti / n_rep / 1e9)
  }
  
  mvtnorm_res                <- run_n_time(use_mvtnorm(n_sample))
  
  TruncatedNormal_res        <- run_n_time(use_trunc_norm(n_sample))
  
  mvndst_no_aprx_res_Korobov <- 
    run_n_time(use_mvndst(FALSE, method = 0L, use_tilting = FALSE))
  mvndst_w_aprx_res_Korobov  <- 
    run_n_time(use_mvndst(TRUE , method = 0L, use_tilting = FALSE))
  mvndst_no_aprx_res_Sobol   <- 
    run_n_time(use_mvndst(FALSE, method = 1L, use_tilting = FALSE))
  mvndst_w_aprx_res_Sobol    <- 
    run_n_time(use_mvndst(TRUE , method = 1L, use_tilting = FALSE))
  
  mvndst_no_aprx_res_Korobov_tilt <- 
    run_n_time(use_mvndst(FALSE, method = 0L, use_tilting = TRUE))
  mvndst_w_aprx_res_Korobov_tilt  <- 
    run_n_time(use_mvndst(TRUE , method = 0L, use_tilting = TRUE))
  mvndst_no_aprx_res_Sobol_tilt   <- 
    run_n_time(use_mvndst(FALSE, method = 1L, use_tilting = TRUE))
  mvndst_w_aprx_res_Sobol_tilt    <- 
    run_n_time(use_mvndst(TRUE , method = 1L, use_tilting = TRUE))
  
  rbind(mvtnorm            = mvtnorm_res, 
        TruncatedNormal = TruncatedNormal_res,
        `no aprx; Korobov` = mvndst_no_aprx_res_Korobov, 
        `no aprx; Sobol` = mvndst_no_aprx_res_Sobol, 
        `w/ aprx; Korobov` = mvndst_w_aprx_res_Korobov,
        `w/ aprx; Sobol` = mvndst_w_aprx_res_Sobol,
        
        `no aprx; Korobov (tilt)` = mvndst_no_aprx_res_Korobov_tilt, 
        `no aprx; Sobol (tilt)` = mvndst_no_aprx_res_Sobol_tilt, 
        `w/ aprx; Korobov (tilt)` = mvndst_w_aprx_res_Korobov_tilt,
        `w/ aprx; Sobol (tilt)` = mvndst_w_aprx_res_Sobol_tilt)
}, simplify = "array")

dimnames(sim_res) <- 
  setNames(c(dimnames(sim_res)[1:2], list(ds)), 
           c("Method", "Metric", "Dimension"))
```

The relative errors plotted against the dimension is shown below:

```{r err_botev_example, fig.height = knitr::opts_chunk$get("fig.height") * 4/3}
# the errors for each method and dimension
sim_res[, "SE", ]

# plot the errors
par(mar = c(5, 5, 1, 1), cex = .8)
matplot(ds, t(sim_res[, "SE", ]), type = "p", log = "y", 
        pch = 1:dim(sim_res)[1], xlab = "Dimension", ylab = "Relative error", 
        col = "black", bty = "l")
matlines(ds, t(sim_res[, "SE", ]), col = "black", lty = 2)
legend("bottomright", bty = "n", pch = 1:dim(sim_res)[1], 
       legend = dimnames(sim_res)[[1]])
grid()
```

A similar plot for the average estimation time is shown below.

```{r time_botev_example, fig.height = knitr::opts_chunk$get("fig.height") * 4/3}
# the computation time for each method and dimension
sim_res[, "time", ]

# plot the computation time
par(mar = c(5, 5, 1, 1), cex = .8)
matplot(ds, t(sim_res[, "time", ]), type = "p", log = "y", 
        pch = 1:dim(sim_res)[1], xlab = "Dimension", ylab = "Time", 
        col = "black", bty = "l")
matlines(ds, t(sim_res[, "time", ]), col = "black", lty = 2)
legend("bottomright", bty = "n", pch = 1:dim(sim_res)[1], 
       legend = dimnames(sim_res)[[1]])
grid()
```


## References
